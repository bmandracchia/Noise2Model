{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core\n",
    "\n",
    "> Core functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp layers.logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastai.vision.all import torch, nn, np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ConditionalLinear(nn.Module):\n",
    "    def __init__(self, device='cuda', name='linear_transformation'):\n",
    "        super(ConditionalLinear, self).__init__()\n",
    "        self.name = name\n",
    "\n",
    "        self.iso_vals = torch.tensor([100, 400, 800, 1600, 3200], dtype=torch.float32, device=device)\n",
    "        self.cam_vals = torch.tensor([0, 1, 2, 3, 4], dtype=torch.float32, device=device)  # 'IP', 'GP', 'S6', 'N6', 'G4'\n",
    "\n",
    "        self.log_scale = nn.Parameter(torch.zeros(25), requires_grad=True)\n",
    "        self.bias = nn.Parameter(torch.zeros(25), requires_grad=True)\n",
    "\n",
    "    def _inverse(self, z, **kwargs):\n",
    "        gain_one_hot = self.iso_vals == torch.mean(kwargs['iso'], dim=[1, 2, 3]).unsqueeze(1)\n",
    "        iso = gain_one_hot.nonzero()[:, 1]\n",
    "        cam_one_hot = self.cam_vals == torch.mean(kwargs['cam'], dim=[1, 2, 3]).unsqueeze(1)\n",
    "        cam = cam_one_hot.nonzero()[:, 1]\n",
    "        iso_cam = iso * 5 + cam\n",
    "        iso_cam = torch.arange(0, 25).cuda() == iso_cam.unsqueeze(1)\n",
    "\n",
    "        log_scale = self.log_scale.unsqueeze(0).repeat_interleave(z.shape[0], dim=0)[iso_cam]\n",
    "        bias = self.bias.unsqueeze(0).repeat_interleave(z.shape[0], dim=0)[iso_cam]\n",
    "\n",
    "        x = (z - bias.reshape((-1, 1, 1, 1))) / torch.exp(log_scale.reshape((-1, 1, 1, 1)))\n",
    "        return x\n",
    "\n",
    "    def _forward_and_log_det_jacobian(self, x, **kwargs):\n",
    "        gain_one_hot = self.iso_vals == torch.mean(kwargs['iso'], dim=[1, 2, 3]).unsqueeze(1)\n",
    "        iso = gain_one_hot.nonzero()[:, 1]\n",
    "        cam_one_hot = self.cam_vals == torch.mean(kwargs['cam'], dim=[1, 2, 3]).unsqueeze(1)\n",
    "        cam = cam_one_hot.nonzero()[:, 1]\n",
    "        iso_cam = iso * 5 + cam\n",
    "        iso_cam = torch.arange(0, 25).cuda() == iso_cam.unsqueeze(1)\n",
    "\n",
    "        log_scale = self.log_scale.unsqueeze(0).repeat_interleave(x.shape[0], dim=0)[iso_cam]\n",
    "        bias = self.bias.unsqueeze(0).repeat_interleave(x.shape[0], dim=0)[iso_cam]\n",
    "        \n",
    "        z = x * torch.exp(log_scale.reshape((-1, 1, 1, 1))) + bias.reshape((-1, 1, 1, 1))\n",
    "        log_abs_det_J_inv = log_scale * np.prod(x.shape[1:])\n",
    "\n",
    "        return z, log_abs_det_J_inv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ConditionalLinearExp2(nn.Module):\n",
    "    def __init__(self, device='cuda', name='linear_transformation_exp2'):\n",
    "        super(ConditionalLinearExp2, self).__init__()\n",
    "        self.name = name\n",
    "\n",
    "        self.iso_vals = torch.tensor([100, 400, 800, 1600, 3200], dtype=torch.float32, device=device)\n",
    "        self.cam_vals = torch.tensor([0, 1, 2, 3, 4], dtype=torch.float32, device=device)  # 'IP', 'GP', 'S6', 'N6', 'G4'\n",
    "\n",
    "        self.log_scale = nn.Parameter(torch.zeros(25, 3), requires_grad=True)\n",
    "        self.bias = nn.Parameter(torch.zeros(25, 3), requires_grad=True)\n",
    "\n",
    "    def _inverse(self, z, **kwargs):\n",
    "        gain_one_hot = self.iso_vals == torch.mean(kwargs['iso'], dim=[1, 2, 3]).unsqueeze(1)\n",
    "        iso = gain_one_hot.nonzero()[:, 1]\n",
    "        cam_one_hot = self.cam_vals == torch.mean(kwargs['cam'], dim=[1, 2, 3]).unsqueeze(1)\n",
    "        cam = cam_one_hot.nonzero()[:, 1]\n",
    "        iso_cam = iso * 5 + cam\n",
    "        iso_cam = torch.arange(0, 25).cuda() == iso_cam.unsqueeze(1)\n",
    "\n",
    "        log_scale = self.log_scale.unsqueeze(0).repeat_interleave(z.shape[0], dim=0)[iso_cam]\n",
    "        bias = self.bias.unsqueeze(0).repeat_interleave(z.shape[0], dim=0)[iso_cam]\n",
    "\n",
    "        x = (z - bias.reshape((-1, z.shape[1], 1, 1))) / torch.exp(log_scale.reshape((-1, z.shape[1], 1, 1)))\n",
    "        return x\n",
    "    def _forward_and_log_det_jacobian(self, x, **kwargs):\n",
    "        gain_one_hot = self.iso_vals == torch.mean(kwargs['iso'], dim=[1, 2, 3]).unsqueeze(1)\n",
    "        iso = gain_one_hot.nonzero()[:, 1]\n",
    "        cam_one_hot = self.cam_vals == torch.mean(kwargs['cam'], dim=[1, 2, 3]).unsqueeze(1)\n",
    "        cam = cam_one_hot.nonzero()[:, 1]\n",
    "        iso_cam = iso * 5 + cam\n",
    "        iso_cam = torch.arange(0, 25).cuda() == iso_cam.unsqueeze(1)\n",
    "\n",
    "        log_scale = self.log_scale.unsqueeze(0).repeat_interleave(x.shape[0], dim=0)[iso_cam]\n",
    "        bias = self.bias.unsqueeze(0).repeat_interleave(x.shape[0], dim=0)[iso_cam]\n",
    "        z = x * torch.exp(log_scale.reshape((-1, x.shape[1], 1, 1))) + bias.reshape((-1, x.shape[1], 1, 1))\n",
    "        log_abs_det_J_inv = torch.sum(log_scale * np.prod(x.shape[2:]), dim=1)\n",
    "\n",
    "        return z, log_abs_det_J_inv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Gamma(nn.Module):\n",
    "    def __init__(self, device='cuda', name='gamma'):\n",
    "        super(Gamma, self).__init__()\n",
    "        self.name = name\n",
    "        self.gamma = nn.Parameter(torch.tensor(2.2), requires_grad=True)\n",
    "        self.constant = 0.00005\n",
    "\n",
    "    def _inverse(self, z, **kwargs):\n",
    "        z = z.clamp(min=0)\n",
    "        # print(torch.min(z), torch.max(z))\n",
    "        x = z**(1/self.gamma)\n",
    "        x -= self.constant\n",
    "        return x\n",
    "\n",
    "    def _forward_and_log_det_jacobian(self, x, **kwargs):\n",
    "        x += self.constant\n",
    "        z = x**self.gamma\n",
    "        log_abs_det_J_inv = torch.sum(torch.log(self.gamma) + (self.gamma-1)*torch.log(x), dim=[1, 2, 3])\n",
    "\n",
    "        if 'writer' in kwargs.keys():\n",
    "            kwargs['writer'].add_scalar('model/' + self.name + '_train', self.gamma, kwargs['step'])\n",
    "\n",
    "        return z, log_abs_det_J_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "n2m",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
