{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer\n",
    "\n",
    "> Train functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import os \n",
    "import time\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange\n",
    "from importlib import import_module\n",
    "import math\n",
    "\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from Noise2Model.loss import Loss\n",
    "from Noise2Model.data import get_dataset_class\n",
    "from Noise2Model.models import get_model_class\n",
    "from Noise2Model.utils import StandardNormal, FileManager, Logger, setup_determinism, rot_hflip_img, np2tensor, np2tensor_multi, tensor2np, get_file_name_from_path, psnr, ssim, make_predefiend_1d_to_2d, load_numpy_from_raw, save_img, AverageMeter, kl_div_3_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch import randn as torch_randn\n",
    "from fastai.vision.all import test_eq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "trainer_class_dict = {}\n",
    "\n",
    "def regist_trainer(trainer):\n",
    "    trainer_name = trainer.__name__.lower()\n",
    "    assert not trainer_name in trainer_class_dict, 'there is already registered dataset: %s in trainer_dict.' % trainer_name\n",
    "    trainer_class_dict[trainer_name] = trainer\n",
    "\n",
    "    return trainer\n",
    "\n",
    "def get_trainer_class(trainer_name:str):\n",
    "    trainer_name = trainer_name.lower()\n",
    "    return trainer_class_dict[trainer_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class BaseTrainer():\n",
    "    '''\n",
    "    Base trainer class to implement other trainer classes.\n",
    "    below function should be implemented in each of trainer class.\n",
    "    '''\n",
    "    def test(self):\n",
    "        raise NotImplementedError('define this function for each trainer')\n",
    "    def validation(self):\n",
    "        raise NotImplementedError('define this function for each trainer')\n",
    "    def export(self):\n",
    "        raise NotImplementedError('define this function for each trainer')\n",
    "    def _set_module(self):\n",
    "        # return dict form with model name.\n",
    "        raise NotImplementedError('define this function for each trainer')\n",
    "    def _set_optimizer(self):\n",
    "        # return dict form with each coresponding model name.\n",
    "        raise NotImplementedError('define this function for each trainer')\n",
    "    def _forward_fn(self, module, loss, data):\n",
    "        # forward with model, loss function and data.\n",
    "        # return output of loss function.\n",
    "        raise NotImplementedError('define this function for each trainer')\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.session_name = config.LOG.session_name\n",
    "\n",
    "        self.checkpoint_folder = 'checkpoint'\n",
    "\n",
    "        # get file manager and logger class\n",
    "        self.file_manager = FileManager(self.session_name, config.DATA.output_path)\n",
    "        self.logger = Logger()        \n",
    "        self.config = config\n",
    "        self.device = None\n",
    "        \n",
    "    def _log_configs(self, config, prefix=\"\"):\n",
    "        for key, value in config.items():\n",
    "            if isinstance(value, dict):\n",
    "                self._log_configs(value, prefix+\".\"+key)\n",
    "            else:\n",
    "                msg = '{}.{}: {}'.format(prefix, key, value)\n",
    "                self.logger.highlight(msg)\n",
    "\n",
    "    def train(self):\n",
    "        # initializing\n",
    "        self._before_train()\n",
    "\n",
    "        # warmup\n",
    "        if self.epoch == 1 and self.config.TRAIN.warmup:\n",
    "            self._warmup()\n",
    "\n",
    "        # training\n",
    "        for self.epoch in range(self.epoch, self.max_epoch+1):\n",
    "            self._before_epoch()\n",
    "            self._run_epoch()\n",
    "            self._after_epoch()\n",
    "        \n",
    "        self._after_train()\n",
    "        \n",
    "    def _eval_mode(self):\n",
    "        for key in self.model:\n",
    "            self.model[key].eval()\n",
    "\n",
    "    def _train_mode(self):\n",
    "        for key in self.model:\n",
    "            self.model[key].train()\n",
    "                    \n",
    "    def _before_epoch(self):\n",
    "        self._set_status('epoch %04d/%04d'%(self.epoch, self.max_epoch))\n",
    "\n",
    "        # make dataloader iterable.\n",
    "        self.train_dataloader_iter = {}\n",
    "        for key in self.train_dataloader:\n",
    "            self.train_dataloader_iter[key] = iter(self.train_dataloader[key])\n",
    "\n",
    "        # model training mode\n",
    "        self._train_mode()\n",
    "        \n",
    "    def _run_epoch(self):\n",
    "        for self.iter in range(1, self.max_iter+1):\n",
    "            self._before_step()\n",
    "            self._run_step()\n",
    "            self._after_step()\n",
    "\n",
    "    def _after_epoch(self):\n",
    "        # save checkpoint\n",
    "        if self.epoch >= self.config.CKPT.start_epoch:\n",
    "            if (self.epoch-self.config.CKPT.start_epoch)%self.config.CKPT.interval_epoch == 0:\n",
    "                self.save_checkpoint()\n",
    "\n",
    "        # validation\n",
    "        if self.config.DATA.validation_dataset_path is not None:\n",
    "            if self.epoch >= self.config.VALIDATION.start_epoch:\n",
    "                if (self.epoch-self.config.VALIDATION.start_epoch) % self.config.VALIDATION.interval_epoch == 0:\n",
    "                    self._eval_mode()\n",
    "                    self._set_status('val %03d'%self.epoch)\n",
    "                    self.validation()\n",
    "                    \n",
    "    def _after_train(self):\n",
    "        # finish message\n",
    "        self.logger.highlight(self.logger.get_finish_msg())\n",
    "                    \n",
    "    def save_checkpoint(self):\n",
    "        checkpoint_name = self._checkpoint_name(self.epoch)\n",
    "        if len(self.device) > 1:\n",
    "            torch.save({'epoch': self.epoch,\n",
    "                        'model_weight': {key:self.model[key].module.state_dict() for key in self.model},\n",
    "                        'optimizer_weight': {key:self.optimizer[key].state_dict() for key in self.optimizer}},\n",
    "                        os.path.join(self.file_manager.get_dir(self.checkpoint_folder), checkpoint_name))\n",
    "        else:\n",
    "            torch.save({'epoch': self.epoch,\n",
    "                        'model_weight': {key:self.model[key].state_dict() for key in self.model},\n",
    "                        'optimizer_weight': {key:self.optimizer[key].state_dict() for key in self.optimizer}},\n",
    "                        os.path.join(self.file_manager.get_dir(self.checkpoint_folder), checkpoint_name))\n",
    "            \n",
    "    def load_checkpoint(self, load_epoch=0, name=None):\n",
    "        if name is None:\n",
    "            # if scratch, return\n",
    "            if load_epoch == 0: return\n",
    "            # load from local checkpoint folder\n",
    "            file_name = os.path.join(self.file_manager.get_dir(self.checkpoint_folder), self._checkpoint_name(load_epoch))\n",
    "        else:\n",
    "            # load from global checkpoint folder\n",
    "            file_name = os.path.join('./ckpt', name)\n",
    "        \n",
    "        # check file exist\n",
    "        assert os.path.isfile(file_name), 'there is no checkpoint: %s'%file_name\n",
    "\n",
    "        # load checkpoint (epoch, model_weight, optimizer_weight)\n",
    "        saved_checkpoint = torch.load(file_name)\n",
    "        self.epoch = saved_checkpoint['epoch']\n",
    "        for key in self.module:\n",
    "            self.module[key].load_state_dict(saved_checkpoint['model_weight'][key])\n",
    "        if hasattr(self, 'optimizer'):\n",
    "            for key in self.optimizer:\n",
    "                self.optimizer[key].load_state_dict(saved_checkpoint['optimizer_weight'][key])\n",
    "\n",
    "        # print message \n",
    "        self.logger.note('[%s] model loaded : %s'%(self.status, file_name))\n",
    "\n",
    "    def _checkpoint_name(self, epoch, extension='pth'):\n",
    "        return self.session_name + '_%03d'%epoch + f'.{extension}'\n",
    "        \n",
    "    def set_device(self, device):\n",
    "        assert isinstance(device, str)\n",
    "        self.device = [int(i) for i in device.split(',')] \n",
    "\n",
    "    def _set_loss(self):\n",
    "        self.loss = Loss(self.config.TRAIN.loss, self.config.TRAIN.tmp_info)\n",
    "    \n",
    "    def _before_train(self):\n",
    "        # setup determinism\n",
    "        if self.config.BASE.seed > 0:\n",
    "            setup_determinism(self.config.BASE.seed)       \n",
    "\n",
    "        # initialing\n",
    "        self.module = self._set_module()\n",
    "\n",
    "        # training dataset loader\n",
    "        self.logger.info('Prepare training dataloader...')\n",
    "        self.train_dataloader = self._set_dataloader(\n",
    "            self.config.DATA.train_dataset, \n",
    "            self.config.DATA.train_dataset_path, \n",
    "            self.config.DATA.TRAIN_DATALOADER, \n",
    "            batch_size=self.config.DATA.batch_size, \n",
    "            shuffle=True, \n",
    "            num_workers=self.config.DATA.threads\n",
    "            )\n",
    "        self.logger.info('Done!')\n",
    "\n",
    "        # validation dataset loader\n",
    "        self.logger.info('Prepare validation dataloader...')\n",
    "        self.val_dataloader = self._set_dataloader(\n",
    "            self.config.DATA.validation_dataset, \n",
    "            self.config.DATA.validation_dataset_path, \n",
    "            self.config.DATA.VALIDATION_DATALOADER, \n",
    "            batch_size=1, \n",
    "            shuffle=False, \n",
    "            num_workers=self.config.DATA.threads\n",
    "            )\n",
    "        self.logger.info('Done!')\n",
    "\n",
    "        # other configuration\n",
    "        self.max_epoch = self.config.TRAIN.max_epochs\n",
    "        self.epoch = self.start_epoch = 1\n",
    "        max_len = self.train_dataloader['dataset'].dataset.__len__() # base number of iteration works for dataset named 'dataset'\n",
    "        self.max_iter = math.ceil(max_len / self.config.DATA.batch_size)\n",
    "\n",
    "        self._set_loss()\n",
    "        self.loss_dict = {'count':0}\n",
    "        self.tmp_info = {}\n",
    "        self.loss_log = []\n",
    "\n",
    "        # set optimizer\n",
    "        self.optimizer = self._set_optimizer()\n",
    "        for opt in self.optimizer.values():\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        # resume\n",
    "        if self.config.BASE.resume:\n",
    "            # find last checkpoint\n",
    "            load_epoch = self._find_last_epoch()\n",
    "\n",
    "            # load last checkpoint\n",
    "            self.load_checkpoint(load_epoch)\n",
    "            self.epoch = load_epoch+1\n",
    "\n",
    "            # logger initialization\n",
    "            self.logger = Logger((self.max_epoch, self.max_iter), log_dir=self.file_manager.get_dir(''), log_file_option='a')\n",
    "        else:\n",
    "            # logger initialization\n",
    "            self.logger = Logger((self.max_epoch, self.max_iter), log_dir=self.file_manager.get_dir(''), log_file_option='w')\n",
    "\n",
    "        # wrapping and device setting\n",
    "        assert len(self.device) > 0, \"There is not available device.\"\n",
    "        if len(self.device) > 1:\n",
    "            # model to GPU\n",
    "            self.model = {key: nn.DataParallel(self.module[key], self.device).cuda() for key in self.module}\n",
    "            # optimizer to GPU\n",
    "            for optim in self.optimizer.values():\n",
    "                for state in optim.state.values():\n",
    "                    for k, v in state.items():\n",
    "                        if isinstance(v, torch.Tensor):\n",
    "                            state[k] = v.cuda()\n",
    "        else:\n",
    "            os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "            os.environ[\"CUDA_VISIBLE_DEVICES\"]= str(self.device[0])\n",
    "            self.model = {key: self.module[key].cuda() for key in self.module}\n",
    "\n",
    "        # start message\n",
    "        #self.logger.info(self.summary())\n",
    "        self.logger.start((self.epoch-1, 0))\n",
    "        self.logger.highlight(self.logger.get_start_msg())\n",
    "        self._log_configs(self.config)\n",
    "\n",
    "    def _warmup(self):\n",
    "        self._set_status('warmup')\n",
    "        self._train_mode()\n",
    "        \n",
    "        # make dataloader iterable.\n",
    "        self.train_dataloader_iter = {}\n",
    "        for key in self.train_dataloader:\n",
    "            self.train_dataloader_iter[key] = iter(self.train_dataloader[key])\n",
    "\n",
    "        warmup_iter = self.config.TRAIN.warmup_iters\n",
    "        if warmup_iter > self.max_iter:\n",
    "            self.logger.info('currently warmup support 1 epoch as maximum. warmup iter is replaced to 1 epoch iteration. %d -> %d' \\\n",
    "                % (warmup_iter, self.max_iter))\n",
    "            warmup_iter = self.max_iter\n",
    "\n",
    "        for self.iter in range(1, warmup_iter+1):\n",
    "            self._adjust_warmup_lr(warmup_iter)\n",
    "            self._before_step()\n",
    "            self._run_step()\n",
    "            self._after_step()\n",
    "            \n",
    "    def _before_step(self):\n",
    "        pass\n",
    "    \n",
    "    def _run_step(self):\n",
    "        # get data (data should be dictionary of Tensors)\n",
    "        data = {}\n",
    "        self._train_mode()\n",
    "        for key in self.train_dataloader_iter:\n",
    "            try:\n",
    "                data[key] = next(self.train_dataloader_iter[key])\n",
    "            except StopIteration:\n",
    "                self.train_dataloader_iter[key] = iter(self.train_dataloader[key])\n",
    "                data[key] = next(self.train_dataloader_iter[key])\n",
    "                            \n",
    "        # to device\n",
    "        if len(self.device) > 0 :\n",
    "            for dataset_key in data:\n",
    "                for key in data[dataset_key]:\n",
    "                    if isinstance(data[dataset_key][key], dict):\n",
    "                        dictdata = data[dataset_key][key]\n",
    "                        for k in dictdata:\n",
    "                            if isinstance(dictdata[k], torch.Tensor):\n",
    "                                dictdata[k] = dictdata[k].cuda()        \n",
    "                    else:\n",
    "                        data[dataset_key][key] = data[dataset_key][key].cuda()\n",
    "\n",
    "        # forward, call losses, backward\n",
    "        losses, tmp_info = self._forward_fn(self.model, self.loss, data)\n",
    "        losses   = {key: losses[key].mean()   for key in losses}\n",
    "        tmp_info = {key: tmp_info[key].mean() for key in tmp_info}\n",
    "\n",
    "        # backward\n",
    "        total_loss = sum(v for v in losses.values())\n",
    "        total_loss.backward()\n",
    "\n",
    "        # optimizer step\n",
    "        for opt in self.optimizer.values():\n",
    "            opt.step()\n",
    "\n",
    "        # zero grad\n",
    "        for opt in self.optimizer.values():\n",
    "            opt.zero_grad(set_to_none=True) \n",
    "\n",
    "        # save losses and tmp_info\n",
    "        for key in losses:\n",
    "            if key != 'count':\n",
    "                if key in self.loss_dict:\n",
    "                    self.loss_dict[key] += float(losses[key])\n",
    "                else:\n",
    "                    self.loss_dict[key] = float(losses[key])\n",
    "        for key in tmp_info:\n",
    "            if key in self.tmp_info:\n",
    "                self.tmp_info[key] += float(tmp_info[key])\n",
    "            else:\n",
    "                self.tmp_info[key] = float(tmp_info[key])\n",
    "        self.loss_dict['count'] += 1\n",
    "\n",
    "    def _after_step(self):\n",
    "        # adjust learning rate\n",
    "        self._adjust_lr()\n",
    "\n",
    "        # print loss\n",
    "        if (self.iter%self.config.LOG.interval_iter==0 and self.iter!=0) or (self.iter == self.max_iter):\n",
    "            self.print_loss()\n",
    "\n",
    "        # print progress\n",
    "        self.logger.print_prog_msg((self.epoch-1, self.iter-1))\n",
    "        \n",
    "    def _adjust_lr(self):\n",
    "        sched = self.config.SCHEDULER.type\n",
    "\n",
    "        if sched == 'step':\n",
    "            '''\n",
    "            step decreasing scheduler\n",
    "            Args:\n",
    "                step_size: step size(epoch) to decay the learning rate\n",
    "                gamma: decay rate\n",
    "            '''\n",
    "            if self.iter == self.max_iter:\n",
    "                args = self.config.SCHEDULER.STEP\n",
    "                if self.epoch % args.step_size == 0:\n",
    "                    for optimizer in self.optimizer.values():\n",
    "                        lr_before = optimizer.param_groups[0]['lr']\n",
    "                        for param_group in optimizer.param_groups:\n",
    "                            param_group[\"lr\"] = lr_before * float(args.gamma)\n",
    "        elif sched == 'linear':\n",
    "            '''\n",
    "            linear decreasing scheduler\n",
    "            Args:\n",
    "                step_size: step size(epoch) to decrease the learning rate\n",
    "                gamma: decay rate for reset learning rate\n",
    "            '''\n",
    "            args = self.config.SCHEDULER.LINEAR\n",
    "            if not hasattr(self, 'reset_lr'):\n",
    "                self.reset_lr = float(self.train_cfg['init_lr']) * float(args.gamma)**((self.epoch-1)//args.step_size)\n",
    "\n",
    "            # reset lr to initial value\n",
    "            if self.epoch % args.step_size == 0 and self.iter == self.max_iter:\n",
    "                self.reset_lr = float(self.train_cfg['init_lr']) * float(args.gamma)**(self.epoch//args.step_size)\n",
    "                for optimizer in self.optimizer.values():\n",
    "                    for param_group in optimizer.param_groups:\n",
    "                        param_group[\"lr\"] = self.reset_lr\n",
    "            # linear decaying\n",
    "            else:\n",
    "                ratio = ((self.epoch + (self.iter)/self.max_iter - 1) % args['step_size']) / args['step_size']\n",
    "                curr_lr = (1-ratio) * self.reset_lr\n",
    "                for optimizer in self.optimizer.values():\n",
    "                    for param_group in optimizer.param_groups:\n",
    "                        param_group[\"lr\"] = curr_lr\n",
    "        else:\n",
    "            raise RuntimeError('ambiguious scheduler type: {}'.format(sched))\n",
    "    \n",
    "    def _get_current_lr(self):\n",
    "        for first_optim in self.optimizer.values():\n",
    "            for param_group in first_optim.param_groups:\n",
    "                return param_group['lr']\n",
    "            \n",
    "    def print_loss(self):\n",
    "        temporal_loss = 0.\n",
    "        for key in self.loss_dict:\n",
    "            if key != 'count':\n",
    "                    temporal_loss += self.loss_dict[key]/self.loss_dict['count']\n",
    "        self.loss_log += [temporal_loss]\n",
    "        if len(self.loss_log) > 100: self.loss_log.pop(0)\n",
    "\n",
    "        # print status and learning rate\n",
    "        loss_out_str = '[%s] %04d/%04d, lr:%s ∣ '%(self.status, self.iter, self.max_iter, \"{:.1e}\".format(self._get_current_lr()))\n",
    "        global_iter = (self.epoch-1)*self.max_iter + self.iter\n",
    "\n",
    "        # print losses\n",
    "        avg_loss = np.mean(self.loss_log)\n",
    "        loss_out_str += 'avg_100 : %.6f ∣ '%(avg_loss)\n",
    "\n",
    "        for key in self.loss_dict:\n",
    "            if key != 'count':\n",
    "                loss = self.loss_dict[key]/self.loss_dict['count']\n",
    "                loss_out_str += '%s : %.6f ∣ '%(key, loss)\n",
    "                self.loss_dict[key] = 0.\n",
    "\n",
    "        # print temporal information\n",
    "        if len(self.tmp_info) > 0:\n",
    "            loss_out_str += '\\t['\n",
    "            for key in self.tmp_info:\n",
    "                loss_out_str += '  %s : %.2f'%(key, self.tmp_info[key]/self.loss_dict['count'])\n",
    "                self.tmp_info[key] = 0.\n",
    "            loss_out_str += ' ]'\n",
    "\n",
    "        # reset\n",
    "        self.loss_dict['count'] = 0\n",
    "        self.logger.info(loss_out_str)\n",
    "    \n",
    "    def _set_status(self, status:str):\n",
    "        status_len = 15\n",
    "        assert len(status) <= status_len, 'status string cannot exceed %d characters, (now %d)'%(status_len, len(status))\n",
    "\n",
    "        if len(status.split(' ')) == 2:\n",
    "            s0, s1 = status.split(' ')\n",
    "            self.status = '%s'%s0.rjust(status_len//2) + ' '\\\n",
    "                          '%s'%s1.ljust(status_len//2)\n",
    "        else:\n",
    "            sp = status_len - len(status)\n",
    "            self.status = ''.ljust(sp//2) + status + ''.ljust((sp+1)//2)\n",
    "            \n",
    "    def _adjust_warmup_lr(self, warmup_iter):\n",
    "        init_lr = float(self.config.TRAIN.init_lr)\n",
    "        warmup_lr = init_lr * self.iter / warmup_iter\n",
    "\n",
    "        for optimizer in self.optimizer.values():\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group[\"lr\"] = warmup_lr\n",
    "                \n",
    "    def _set_dataloader(self, dataset_class, path, dataset_args, batch_size, shuffle, num_workers):\n",
    "        dataloader = {}\n",
    "        \n",
    "        dataset_args['path'] = path\n",
    "        dataset = get_dataset_class(dataset_class)(**dataset_args)\n",
    "        dataloader['dataset'] = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=False)\n",
    "        return dataloader\n",
    "    \n",
    "    def _set_one_optimizer(self, opt, parameters, lr):\n",
    "        if opt.type == 'SGD':\n",
    "            return torch.optim.SGD(parameters, lr=lr, momentum=float(opt.SGD.momentum), weight_decay=float(opt.SGD.weight_decay))\n",
    "        elif opt.type == 'Adam':\n",
    "            return torch.optim.Adam(parameters, lr=lr, betas=opt.ADAM.betas)\n",
    "        elif opt.type == 'AdamW':\n",
    "            return torch.optim.Adam(parameters, lr=lr, betas=opt.ADAMW.betas, weight_decay=float(opt.ADAMW.weight_decay))\n",
    "        else:\n",
    "            raise RuntimeError()\n",
    "    \n",
    "    def _set_main_module(self):\n",
    "        if len(self.device) > 1:\n",
    "            module = self.model['denoiser'].module\n",
    "        else:\n",
    "            module = self.model['denoiser']\n",
    "            \n",
    "        if hasattr(module, 'denoise'):\n",
    "            self.denoiser = module.denoise\n",
    "        else:\n",
    "            self.denoiser = module\n",
    "            \n",
    "    def _find_last_epoch(self):\n",
    "        checkpoint_list = os.listdir(self.file_manager.get_dir(self.checkpoint_folder))\n",
    "        epochs = [int(ckpt.replace('%s_'%self.session_name, '').replace('.pth', '')) for ckpt in checkpoint_list]\n",
    "        if len(epochs) <= 0: return 0\n",
    "        return max(epochs)\n",
    "            \n",
    "    def _before_test(self, dataset_load):\n",
    "        # initialing\n",
    "        self.module = self._set_module()\n",
    "        self._set_status('test')\n",
    "\n",
    "        # load checkpoint file\n",
    "        ckpt_epoch = self._find_last_epoch() if self.config.TEST.ckpt_epoch == -1 else self.config.TEST.ckpt_epoch\n",
    "        ckpt_name  = self.config.BASE.pretrained if self.config.BASE.pretrained is not None else None\n",
    "        self.load_checkpoint(ckpt_epoch, ckpt_name)\n",
    "        self.epoch = ckpt_epoch # for print or saving file name.\n",
    "\n",
    "        # test dataset loader\n",
    "        if dataset_load:\n",
    "            self.logger.info('Prepare test dataloader...')\n",
    "            self.test_dataloader = self._set_dataloader(\n",
    "                self.config.DATA.test_dataset, \n",
    "                self.config.DATA.test_dataset_path, \n",
    "                self.config.DATA.TEST_DATALOADER, \n",
    "                batch_size=1, \n",
    "                shuffle=False, \n",
    "                num_workers=self.config.DATA.threads\n",
    "                )\n",
    "            self.logger.info('Done!')\n",
    "\n",
    "\n",
    "        # wrapping and device setting\n",
    "        assert len(self.device) > 0, \"There is not available device.\"\n",
    "        if len(self.device) > 1:\n",
    "            # model to GPU\n",
    "            self.model = {key: nn.DataParallel(self.module[key], self.device).cuda() for key in self.module}\n",
    "        else:\n",
    "            os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "            os.environ[\"CUDA_VISIBLE_DEVICES\"]= str(self.device[0])\n",
    "            self.model = {key: self.module[key].cuda() for key in self.module}\n",
    "\n",
    "        # evaluation mode and set status\n",
    "        self._eval_mode()\n",
    "        self._set_status('test %04d'%self.epoch)\n",
    "\n",
    "        # start message\n",
    "        self.logger.highlight(self.logger.get_start_msg())\n",
    "        self._log_configs(self.config)\n",
    "        \n",
    "        # set denoiser\n",
    "        self._set_main_module()\n",
    "\n",
    "        # wrapping denoiser w/ crop test\n",
    "        if self.config.TEST.crop:\n",
    "            crop_fn = self.denoiser\n",
    "            self.denoiser = lambda *input_data: self.crop_test(crop_fn, \n",
    "                                                               *input_data, \n",
    "                                                               size=self.config.TEST.CROP.size, \n",
    "                                                               overlap=self.config.TEST.CROP.overlap\n",
    "                                                               )\n",
    "\n",
    "    def _get_determine_patch_indices(self, idx, idy, crop_w, crop_h, img_w, img_h):\n",
    "        # get horizontal info.\n",
    "        if idx + crop_w >= img_w:\n",
    "            start_x = img_w-crop_w\n",
    "            end_x = img_w\n",
    "        else:\n",
    "            start_x=idx\n",
    "            end_x = idx+crop_w\n",
    "        # get vertical info.\n",
    "        if idy + crop_h >= img_h:\n",
    "            start_y = img_h-crop_h\n",
    "            end_y = img_h\n",
    "        else:\n",
    "            start_y=idy\n",
    "            end_y = idy+crop_h\n",
    "        return start_x, end_x, start_y, end_y\n",
    "\n",
    "    def _generate_image_patches(self, img, roi, c_w, c_h, s_x, s_y):\n",
    "        '''\n",
    "        img : original image (tensor)\n",
    "        roi : ROI of original image [top, left, height, width]\n",
    "        c_w : width of the cropped images\n",
    "        c_h : height of the cropped images\n",
    "        s_x : stride (x-axis)\n",
    "        s_y : stride (y-axis)\n",
    "        '''\n",
    "        roi_t, roi_l, roi_h, roi_w = roi\n",
    "        _, _, img_h, img_w = img.shape\n",
    "\n",
    "        assert img_h >= roi_t + roi_h and img_w >= roi_l + roi_w\n",
    "\n",
    "        img_roi = img[:,:,roi_t:roi_t+roi_h, roi_l:roi_l+roi_w]\n",
    "        idx, idy = 0, 0\n",
    "        results = list()\n",
    "        while idy + c_h <= roi_h:\n",
    "            while idx + c_w <= roi_w:\n",
    "                start_x, end_x, start_y, end_y = self._get_determine_patch_indices(idx, idy, c_w, c_h, roi_w, roi_h)\n",
    "                cropped = img_roi[:,:,start_y:end_y, start_x:end_x] # img_roi : C, H, W\n",
    "                results.append(cropped)\n",
    "                idx += s_x\n",
    "            idx = 0\n",
    "            idy += s_y\n",
    "        return results\n",
    "\n",
    "    def _generate_weight_kernel(self, c_w, c_h, stride):\n",
    "        kernel = np.zeros((c_h, c_w), dtype=np.float32)\n",
    "        for i in range(kernel.shape[0]):\n",
    "            for j in range(kernel.shape[1]):\n",
    "                val = float(min(min(i, kernel.shape[0] - 1 - i), min(j, kernel.shape[1] - 1 - j)))\n",
    "                kernel[i, j] = val\n",
    "        kernel += 1.0\n",
    "        kernel = np.minimum(kernel, float(stride))\n",
    "        kernel /= float(stride)\n",
    "        return torch.tensor(kernel)\n",
    "    \n",
    "    def _generate_weights_matrix(self, shape, kernel, c_w, c_h, s_x, s_y, device):\n",
    "        _, _, roi_h, roi_w = shape\n",
    "        weights = torch.zeros(shape, device=device, dtype=torch.float32)\n",
    "        idx, idy = 0, 0\n",
    "        while idy + s_y <= roi_h:\n",
    "            while idx + s_x <= roi_w:\n",
    "                i, j = idy, idx\n",
    "                if j + c_w > roi_w: j = roi_w - c_w\n",
    "                if i + c_h > roi_h: i = roi_h - c_h\n",
    "                weights[:, :, i:i+c_h, j:j+c_w] += kernel\n",
    "                idx += s_x\n",
    "            idx = 0\n",
    "            idy += s_y\n",
    "        return weights\n",
    "                            \n",
    "    def _crop_operator(self, img, fn, roi, c_w, c_h, s_x, s_y):\n",
    "        _, _, roi_h, roi_w = roi\n",
    "        b, _, _, _ = img.shape\n",
    "        weights = None\n",
    "        reconst = None\n",
    "\n",
    "        # generate weights kernel\n",
    "        kernel = self._generate_weight_kernel(c_w, c_h, s_x).to(img.device)\n",
    "\n",
    "        idx, idy = 0, 0\n",
    "        while idy + s_y <= roi_h:\n",
    "            while idx + s_x <= roi_w:\n",
    "                i, j = idy, idx\n",
    "                if j + c_w > roi_w: j = roi_w - c_w\n",
    "                if i + c_h > roi_h: i = roi_h - c_h\n",
    "\n",
    "                noisy = img[:,:,i:i+c_h, j:j+c_w]\n",
    "                denoised = fn(noisy)\n",
    "\n",
    "                if weights is None: weights = self._generate_weights_matrix([denoised.shape[0], denoised.shape[1], roi_h, roi_w], kernel, c_w, c_h, s_x, s_y, img.device)\n",
    "                if reconst is None: reconst = torch.zeros([denoised.shape[0], denoised.shape[1], roi_h, roi_w], device = img.device, dtype=torch.float32)\n",
    "\n",
    "                reconst[:,:,i:i+c_h, j:j+c_w] += denoised * (kernel / weights[:,:,i:i+c_h, j:j+c_w])\n",
    "                idx += s_x\n",
    "            idx = 0 \n",
    "            idy += s_y\n",
    "        return reconst\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def crop_test(self, fn, x, kwargs=None, size=512, overlap=0):\n",
    "        _,_,h,w = x.shape\n",
    "        assert size > overlap, \"Invalid parameter. (size <= overlap)\"\n",
    "        reconst = self._crop_operator(x, fn, [0,0,h,w], size, size, size-overlap, size-overlap)\n",
    "        return reconst\n",
    "    \n",
    "    def test_img(self, image_dir, save_dir='./'):\n",
    "        '''\n",
    "        Inference a single image.\n",
    "        '''      \n",
    "        # load image (noisy)\n",
    "        noisy = None\n",
    "        if image_dir[-4:] == '.raw':\n",
    "            noisy = np2tensor(make_predefiend_1d_to_2d(load_numpy_from_raw(image_dir, 'uint16')).astype(np.float32)).unsqueeze(0)\n",
    "        elif self.config.TEST.imread.lower() == 'gray' or self.config.TEST.imread.lower() == 'grey':\n",
    "            ret, noisy = cv2.imreadmulti(image_dir, flags=cv2.IMREAD_GRAYSCALE|cv2.IMREAD_ANYDEPTH) # it can be multi-stacked image.\n",
    "            assert ret > 0, f\"Failed to load image: {image_dir}\"\n",
    "            noisy = np2tensor_multi(noisy)\n",
    "        else:\n",
    "            noisy = np2tensor(cv2.imread(image_dir)).unsqueeze(0)\n",
    "\n",
    "        # load image (clean, optional)\n",
    "        clean = None\n",
    "        if image_dir.find(\"_N.\") > 0: # if not exist, -1 is returned, \n",
    "            clean_path = image_dir.replace(\"_N.\", \"_CL.\")\n",
    "            clean_path = clean_path if os.path.exists(clean_path) else None\n",
    "            if clean_path[-4:] == '.raw':\n",
    "                clean = np2tensor(make_predefiend_1d_to_2d(load_numpy_from_raw(clean_path, 'uint16')).astype(np.float32)).unsqueeze(0)\n",
    "            elif self.config.TEST.imread.lower() == 'gray' or self.config.TEST.imread.lower() == 'grey':\n",
    "                ret, clean = cv2.imreadmulti(clean_path, flags=cv2.IMREAD_GRAYSCALE|cv2.IMREAD_ANYDEPTH) # it can be multi-stacked image.\n",
    "                assert ret > 0, f\"Failed to load image: {clean_path}\"\n",
    "                clean = np2tensor_multi(clean)\n",
    "            else:\n",
    "                clean = np2tensor(cv2.imread(clean_path)).unsqueeze(0)\n",
    "\n",
    "        if len(self.device) > 0:\n",
    "            noisy = noisy.cuda()\n",
    "\n",
    "        # multi-frame input\n",
    "        if self.config.TEST.TEST_DIR.no_input_frames is None: no_input_frames = 1\n",
    "        else: no_input_frames = self.config.TEST.TEST_DIR.no_input_frames\n",
    "\n",
    "        denoised = None\n",
    "        if noisy.shape[0] > 1 : pbar = tqdm(range(noisy.shape[0] - no_input_frames + 1))\n",
    "        else : pbar = range(noisy.shape[0] - no_input_frames + 1)\n",
    "        for batch_idx in pbar:\n",
    "            noisy_batch = noisy[batch_idx:batch_idx+no_input_frames, ...]\n",
    "            noisy_batch = rearrange(noisy_batch, 'b c h w -> 1 (b c) h w') # batch size is always 1. number of multi-frame denotes number of channels.\n",
    "\n",
    "            torch.cuda.synchronize()\n",
    "            tic = time.time()\n",
    "\n",
    "            # denoising\n",
    "            if denoised is None : denoised = self.denoiser(noisy_batch, {'num_bits':self.config.TEST.using_bits}).cpu()\n",
    "            else: denoised = torch.cat([denoised, self.denoiser(noisy_batch, {'num_bits':self.config.TEST.using_bits}).cpu()], dim=0)\n",
    "\n",
    "            torch.cuda.synchronize()\n",
    "            toc = time.time()\n",
    "            time_span_ms = (toc-tic)*1000.\n",
    "            if isinstance(pbar, tqdm):\n",
    "                pbar.set_description('Time span (ms): %.2f'%(time_span_ms))\n",
    "            else:\n",
    "                self.logger.note('Time span (ms): %.2f'%(time_span_ms))\n",
    "\n",
    "        # calculate PSNR and SSIM, if possible\n",
    "        PSNR, SSIM = None, None\n",
    "        if clean: # if not exist, -1 is returned, \n",
    "            #max_val = torch.max(denoised).item() if torch.max(denoised) > torch.max(clean) else torch.max(clean).item()\n",
    "            PSNR = psnr(denoised, clean, max_val=2**self.config.TEST.using_bits-1 if self.config.TEST.using_bits else 255),  \n",
    "            SSIM = ssim(denoised, clean, data_range=2**self.config.TEST.using_bits-1 if self.config.TEST.using_bits else 255)\n",
    "            \n",
    "        # post-process\n",
    "        denoised *= self.config.TEST.scale\n",
    "        denoised += self.config.TEST.add_con\n",
    "        if self.config.TEST.floor: denoised = torch.floor(denoised)\n",
    "\n",
    "        # save image\n",
    "        denoised = tensor2np(denoised)\n",
    "        #denoised = denoised.squeeze(0)\n",
    "        if self.config.TEST.floor: denoised = np.clip(denoised, a_min=0, a_max=2**16-1).astype(np.uint16)\n",
    "        name = get_file_name_from_path(image_dir)\n",
    "        if PSNR and SSIM:\n",
    "            save_img(save_dir, name.replace('_N','_DN')+'_psnr_%.3f_ssim_%.5f'%(PSNR, SSIM)+'.'+self.config.TEST.save_ext, denoised)\n",
    "        else:\n",
    "            save_img(save_dir, name + '_DN' + '.'+self.config.TEST.save_ext, denoised)\n",
    "            if self.config.TEST.TEST_DIR.save_original_img:\n",
    "                noisy_tmp = tensor2np(noisy[-denoised.shape[0]:, ...].cpu().squeeze(0))*self.config.TEST.scale\n",
    "                if self.config.TEST.floor: noisy_tmp = np.floor(noisy_tmp+self.config.TEST.add_con).astype(np.uint16)\n",
    "                save_img(save_dir, name + '_N' + '.'+self.config.TEST.save_ext, noisy_tmp)\n",
    "\n",
    "        # print message\n",
    "        if PSNR and SSIM:\n",
    "            self.logger.note('[%s] saved : %s (psnr: %.3f, ssim: %.5f'% \\\n",
    "                             (self.status, os.path.join(save_dir, name.replace('_N','_DN')+'.'+ self.config.TEST.save_ext), PSNR, SSIM))\n",
    "        else:\n",
    "            self.logger.note('[%s] saved : %s'%(self.status, os.path.join(save_dir, name+'_DN.' + self.config.TEST.save_ext)))\n",
    "\n",
    "    def test_dir(self, direc):\n",
    "        '''\n",
    "        Inference all images in the directory.\n",
    "        '''\n",
    "        for ff in [f for f in os.listdir(direc) if os.path.isfile(os.path.join(direc, f))]:\n",
    "            if \"_CL.\" in ff: continue\n",
    "            if ff[-4:] != '.tif' and ff[-5:] != '.tiff' and ff[-4:] != '.raw' and ff[-4:] != '.png' and ff[-4:] != '.jpg': continue\n",
    "            \n",
    "            result_dir_name = f'results_{self.session_name}_epoch{self.epoch}'\n",
    "            if len(self.config.TEST.TEST_DIR.postfix) > 0:\n",
    "                result_dir_name += f'_{self.config.TEST.TEST_DIR.postfix}' \n",
    "\n",
    "            os.makedirs(os.path.join(direc, result_dir_name), exist_ok=True)\n",
    "            self.test_img(os.path.join(direc, ff), os.path.join(direc, result_dir_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NoiseFlowGAN Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@regist_trainer\n",
    "class NoiseFlowGANTrainer(BaseTrainer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        if config.TRAIN.NOISE_GENERATOR_TRAINER:\n",
    "            self.trainer_config = config.TRAIN.NOISE_GENERATOR_TRAINER\n",
    "\n",
    "    def _set_module(self):\n",
    "        kwargs_gen_flow, kwargs_gen_generator, kwargs_critic = None, None, None\n",
    "        if self.config.BASE.model.lower() == 'nmflowgan':\n",
    "            kwargs_gen_flow = self.config.MODEL.NMFLOW\n",
    "            kwargs_gen_generator = self.config.MODEL.UNET\n",
    "            kwargs_critic = self.config.MODEL.NMFLOWGAN_CRITIC\n",
    "        else:\n",
    "            assert False, \"Invalid model name.\"\n",
    "\n",
    "        module = {}\n",
    "        if kwargs_gen_flow is None or kwargs_gen_generator is None: \n",
    "            module['generator'] = get_model_class(self.config.BASE.model + \"generator\")(dict(), dict())\n",
    "        else: \n",
    "            module['generator'] = get_model_class(self.config.BASE.model+ \"generator\")(kwargs_gen_generator, kwargs_gen_flow)\n",
    "\n",
    "        if kwargs_critic is None: \n",
    "            module['critic'] = get_model_class(self.config.BASE.model + \"critic\")(dict())\n",
    "        else: \n",
    "            module['critic'] = get_model_class(self.config.BASE.model+ \"critic\")(**kwargs_critic)\n",
    "\n",
    "        return module\n",
    "    \n",
    "    def _set_optimizer(self):\n",
    "        optimizer = {}\n",
    "        for key in self.module:\n",
    "            optimizer[key] = self._set_one_optimizer(opt        = self.config.OPTIMIZER, \n",
    "                                                     parameters = self.module[key].parameters(), \n",
    "                                                     lr         = float(self.config.TRAIN.init_lr))\n",
    "        return optimizer\n",
    "    \n",
    "    def _forward_fn(self, module, loss, data, mode='generator'):\n",
    "        assert 'clean' in data['dataset'] and 'noisy' in data['dataset']\n",
    "\n",
    "        input_data = [data['dataset'][arg] for arg in self.config.MODEL.input_type] # noisy, clean, kwargs..\n",
    "        z, ldj, noisy, px_noisy = module['generator'](*input_data)\n",
    "        model_output = {'z':z, 'ldj':ldj, 'noisy':noisy, 'px_noisy': px_noisy} \n",
    "\n",
    "        model_output['fake'] = model_output['noisy']\n",
    "        model_output['real'] = data['dataset']['noisy']\n",
    "        model_output['critic_noise'] = self.trainer_config.critic_noise\n",
    "        if model_output['critic_noise']:\n",
    "            #REMARKS: If this part is changed, the logic for calculating the gradient penalty must also be changed.\n",
    "            fake_noise = (model_output['fake']-data['dataset']['clean']).requires_grad_(True)\n",
    "            real_noise = (model_output['real']-data['dataset']['clean']).requires_grad_(True)\n",
    "            model_output['critic_fake'] = module['critic'](torch.cat([fake_noise, data['dataset']['clean']],dim=1).requires_grad_(True))\n",
    "            model_output['critic_real'] = module['critic'](torch.cat([real_noise, data['dataset']['clean']],dim=1).requires_grad_(True))\n",
    "        else:\n",
    "            model_output['critic_fake'] = module['critic'](model_output['fake'])\n",
    "            model_output['critic_real'] = module['critic'](model_output['real'])\n",
    "\n",
    "        model_output['training_mode'] = mode\n",
    "        \n",
    "        losses, tmp_info = loss(input_data, model_output, data['dataset'], module, ratio=1.)\n",
    "        return losses, tmp_info\n",
    "\n",
    "    def _set_main_module(self):\n",
    "        if len(self.device) > 1:\n",
    "            module = self.model['generator'].module\n",
    "        else:\n",
    "            module = self.model['generator']\n",
    "            \n",
    "        if hasattr(module, 'sample'):\n",
    "            self.sampler = module.sample\n",
    "        else:\n",
    "            self.sampler = module\n",
    "\n",
    "    def test(self):\n",
    "        dataset_load = (self.config.TEST.test_img is None) and (self.config.TEST.test_dir is None) \n",
    "        self._before_test(dataset_load)\n",
    "        \n",
    "        # set image save path\n",
    "        for i in range(60):\n",
    "            test_time = datetime.datetime.now().strftime('%m-%d-%H-%M') + '-%02d'%i\n",
    "            img_save_path = 'img/test_%s_%03d_%s' % (self.config.DATA.test_dataset, self.epoch, test_time)\n",
    "            if not self.file_manager.is_dir_exist(img_save_path): break\n",
    "\n",
    "        kld = None\n",
    "        if self.config.TEST.test_img is not None:\n",
    "            self.test_img(self.config.TEST.test_img, self.config.TEST.save_dir)\n",
    "            exit()\n",
    "        elif self.config.TEST.test_dir is not None:\n",
    "            self.test_dir(self.config.TEST.test_dir)\n",
    "        else:\n",
    "            kld = self._test_dataloader_process(    dataloader    = self.test_dataloader,\n",
    "                                                    add_con       = 0.  if not 'add_con' in self.config.TEST else self.config.TEST.add_con,\n",
    "                                                    floor         = False if not 'floor' in self.config.TEST else self.config.TEST.floor,   \n",
    "                                                    scale         = 1. if not 'scale' in self.config.TEST else self.config.TEST.scale,\n",
    "                                                    using_bits    = None if not 'using_bits' in self.config.TEST else self.config.TEST.using_bits,\n",
    "                                                    img_save_path = img_save_path,\n",
    "                                                    img_save      = self.config.TEST.save_image)\n",
    "        if kld is not None:\n",
    "            with open(os.path.join(self.file_manager.get_dir(img_save_path), 'kld-%.result'%(kld)), 'w') as f:\n",
    "                f.write('KLD: %f'%(kld))\n",
    "\n",
    "    def validation(self):\n",
    "        # set denoiser\n",
    "        self._set_main_module()\n",
    "\n",
    "        # make directories for image saving\n",
    "        img_save_path = 'img/val_%03d' % self.epoch\n",
    "        self.file_manager.make_dir(img_save_path)\n",
    "        \n",
    "        kld = self._test_dataloader_process(    dataloader    = self.val_dataloader,\n",
    "                                                add_con       = 0.  if not 'add_con' in self.config.VALIDATION else self.config.VALIDATION.add_con,\n",
    "                                                floor         = False if not 'floor' in self.config.VALIDATION else self.config.VALIDATION.floor,   \n",
    "                                                scale         = 1. if not 'scale' in self.config.VALIDATION else self.config.VALIDATION.scale,\n",
    "                                                using_bits    = None if not 'using_bits' in self.config.VALIDATION else self.config.VALIDATION.using_bits,\n",
    "                                                img_save_path = img_save_path,\n",
    "                                                img_save      = self.config.VALIDATION.save_image)\n",
    "        if kld is not None:\n",
    "            with open(os.path.join(self.file_manager.get_dir(img_save_path), 'kld-%.3f.result'%(kld)), 'w') as f:\n",
    "                f.write('KLD: %f'%(kld))\n",
    "\n",
    "    def _test_dataloader_process(self, dataloader, add_con=0., floor=False, scale=1., using_bits=None, img_save=True, img_save_path=None, info=True):\n",
    "        # make directory\n",
    "        self.file_manager.make_dir(img_save_path)\n",
    "        kld_avg = AverageMeter()\n",
    "        for idx, data in enumerate(dataloader['dataset']):\n",
    "            # to device\n",
    "            kwargs = dict()\n",
    "            if len(self.device) > 0:\n",
    "                for key in data:\n",
    "                    if isinstance(data[key], dict):\n",
    "                        dictdata = data[key]\n",
    "                        for k in dictdata:\n",
    "                            if isinstance(dictdata[k], torch.Tensor):\n",
    "                                kwargs[k] = dictdata[k] = dictdata[k].cuda()\n",
    "                    else:\n",
    "                        kwargs[key] = data[key] = data[key].cuda()\n",
    "            \n",
    "            #generated = self.module['noise_model'](*[data[arg] for arg in self.config.MODEL.input_type]).detach()\n",
    "            fake = self.sampler(kwargs=kwargs).detach() \n",
    "            \n",
    "            # evaluation\n",
    "            real_noise=None\n",
    "            if 'real_noisy' in data:\n",
    "                real_noise = data['real_noisy']-data['clean']\n",
    "                generated = fake - data['clean']\n",
    "                # KLD\n",
    "                quantization_bins=2**using_bits if using_bits is not None else 16\n",
    "                kld = kl_div_3_data(\n",
    "                    tensor2np(real_noise).flatten(), \n",
    "                    tensor2np(generated).flatten(),\n",
    "                    None,\n",
    "                    -quantization_bins-4, # add padding\n",
    "                    quantization_bins+5 # add padding\n",
    "                    )\n",
    "                kld_avg.update(kld)\n",
    "\n",
    "            # apply scale\n",
    "            if scale is not None:\n",
    "                for key, value in data.items():\n",
    "                    data[key] = value * scale\n",
    "                fake = fake * scale\n",
    "\n",
    "            # add constant and floor (if floor is on)\n",
    "            fake += add_con\n",
    "            if floor: fake = torch.floor(fake)\n",
    "\n",
    "            if using_bits==8:\n",
    "                for key, value in data.items():\n",
    "                    if key in ['clean', 'real_noisy', 'noisy', 'syn_noisy']:\n",
    "                        data[key] = torch.clip(value,0,255).type(torch.uint8)\n",
    "                fake = torch.clip(fake,0,255).type(torch.uint8)\n",
    "\n",
    "            # image save\n",
    "            if img_save:\n",
    "                # to cpu\n",
    "                if 'clean' in data:\n",
    "                    clean_img = data['clean'].squeeze(0).cpu()\n",
    "                if 'real_noisy' in data: noisy_img = data['real_noisy']\n",
    "                elif 'syn_noisy' in data: noisy_img = data['syn_noisy']\n",
    "                elif 'noisy' in data: noisy_img = data['noisy']\n",
    "                else: noisy_img = None\n",
    "                if noisy_img is not None: noisy_img = noisy_img.squeeze(0).cpu()\n",
    "                fake_img = fake.squeeze(0).cpu()\n",
    "\n",
    "                # write psnr value on file name\n",
    "                gen_name = '%05d_GEN_%.6f'%(idx, kld) if 'real_noisy' in data else '%05d_GEN'%idx\n",
    "\n",
    "                # imwrite\n",
    "                if 'clean' in data:         self.file_manager.save_img_tensor(img_save_path, '%05d_CL'%idx, clean_img, ext=self.config.TEST.save_ext)\n",
    "                if noisy_img is not None: self.file_manager.save_img_tensor(img_save_path, '%05d_N'%idx, noisy_img, ext=self.config.TEST.save_ext)\n",
    "                self.file_manager.save_img_tensor(img_save_path, gen_name, fake_img, ext=self.config.TEST.save_ext)\n",
    "                \n",
    "            if info:\n",
    "                if 'real_noisy' in data:\n",
    "                    self.logger.note('[%s] testing... %05d/%05d. KLD : %.6f'%(self.status, idx, dataloader['dataset'].__len__(), kld), end='\\r')\n",
    "                else:\n",
    "                    self.logger.note('[%s] testing... %05d/%05d.'%(self.status, idx, dataloader['dataset'].__len__()), end='\\r')\n",
    "                               \n",
    "        # final log msg\n",
    "        if kld_avg.count > 0:\n",
    "            self.logger.val('[%s] Done! KLD: %.6f'%(self.status, kld_avg.avg))\n",
    "        else:\n",
    "            self.logger.val('[%s] Done!'%self.status)\n",
    "            \n",
    "        return kld_avg.avg\n",
    "    \n",
    "    def test_img(self, image_dir, save_dir='./'):\n",
    "        '''\n",
    "        Inference a single image.\n",
    "        '''\n",
    "        # load image\n",
    "        if self.config.TEST.imread.lower() == 'gray' or self.config.TEST.imread.lower() == 'grey':\n",
    "            clean = np2tensor(cv2.imread(image_dir, cv2.IMREAD_GRAYSCALE|cv2.IMREAD_ANYDEPTH).astype(np.float32))\n",
    "        else:\n",
    "            clean = np2tensor(cv2.imread(image_dir))\n",
    "        clean = clean.unsqueeze(0).float()\n",
    "\n",
    "        # to device\n",
    "        if len(self.device) > 0:\n",
    "            clean = clean.cuda()\n",
    "\n",
    "        # forward\n",
    "        kwargs = {'clean':clean}\n",
    "        fake = self.sampler(kwargs) \n",
    "\n",
    "        # post-process\n",
    "        fake += self.config.TEST.add_con\n",
    "        if self.config.TEST.floor: fake = torch.floor(fake)\n",
    "            \n",
    "        # save image\n",
    "        fake = tensor2np(fake)\n",
    "        fake = fake.squeeze(0)\n",
    "        if self.config.TEST.floor: fake = fake.astype(np.uint16)\n",
    "        name = get_file_name_from_path(image_dir)\n",
    "        cv2.imwrite(os.path.join(save_dir, name+'_GEN.'+self.config.TEST.save_ext), fake)\n",
    "\n",
    "        # print message\n",
    "        self.logger.note('[%s] saved : %s'%(self.status, os.path.join(save_dir, name+'_GEN.'+self.config.TEST.save_ext)))\n",
    "      \n",
    "    def _run_step(self):\n",
    "        # get data (data should be dictionary of Tensors)\n",
    "        data = {}\n",
    "        self._train_mode()\n",
    "        for key in self.train_dataloader_iter:\n",
    "            try:\n",
    "                data[key] = next(self.train_dataloader_iter[key])\n",
    "            except StopIteration:\n",
    "                self.train_dataloader_iter[key] = iter(self.train_dataloader[key])\n",
    "                data[key] = next(self.train_dataloader_iter[key])\n",
    "                            \n",
    "        # to device \n",
    "        if len(self.device) > 0 :\n",
    "            for dataset_key in data:\n",
    "                for key in data[dataset_key]:\n",
    "                    if isinstance(data[dataset_key][key], dict):\n",
    "                        dictdata = data[dataset_key][key]\n",
    "                        for k in dictdata:\n",
    "                            if isinstance(dictdata[k], torch.Tensor):\n",
    "                                dictdata[k] = dictdata[k].cuda()        \n",
    "                    else:\n",
    "                        data[dataset_key][key] = data[dataset_key][key].cuda()\n",
    "\n",
    "        # forward, call losses, backward\n",
    "        # generator\n",
    "        losses = dict()\n",
    "        tmp_infos = dict()\n",
    "        for mode in self.module:\n",
    "            if mode == 'generator' and self.iter % self.config.TRAIN.NOISE_GENERATOR_TRAINER.generator_iter_step != 0:\n",
    "                continue\n",
    "\n",
    "            # zero grad\n",
    "            self.optimizer[mode].zero_grad(set_to_none=True) \n",
    "\n",
    "            loss, tmp_info = self._forward_fn(self.model, self.loss, data, mode)\n",
    "            losses.update({f'{mode}_'+key: loss[key].mean() for key in loss})\n",
    "            tmp_infos.update({f'{mode}_'+key: tmp_info[key].mean() for key in tmp_info})\n",
    "\n",
    "            # backward\n",
    "            total_loss = sum(v for v in loss.values())\n",
    "            total_loss.backward()\n",
    "\n",
    "            # optimizer step\n",
    "            self.optimizer[mode].step()\n",
    "\n",
    "        # save losses and tmp_info\n",
    "        for key in losses:\n",
    "            if key != 'count':\n",
    "                if key in self.loss_dict:\n",
    "                    self.loss_dict[key] += float(losses[key])\n",
    "                else:\n",
    "                    self.loss_dict[key] = float(losses[key])\n",
    "        for key in tmp_info:\n",
    "            if key in self.tmp_info:\n",
    "                self.tmp_info[key] += float(tmp_info[key])\n",
    "            else:\n",
    "                self.tmp_info[key] = float(tmp_info[key])\n",
    "        self.loss_dict['count'] += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Denoising Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@regist_trainer\n",
    "class SLDenoisingTrainer(BaseTrainer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        \n",
    "    def test(self):\n",
    "        dataset_load = (self.config.TEST.test_img is None) and (self.config.TEST.test_dir is None) \n",
    "        self._before_test(dataset_load)\n",
    "        \n",
    "        # set image save path\n",
    "        for i in range(60):\n",
    "            test_time = datetime.datetime.now().strftime('%m-%d-%H-%M') + '-%02d'%i\n",
    "            img_save_path = 'img/test_%s_%03d_%s' % (self.config.DATA.test_dataset, self.epoch, test_time)\n",
    "            if not self.file_manager.is_dir_exist(img_save_path): break\n",
    "\n",
    "        psnr = None\n",
    "        ssim = None\n",
    "        if self.config.TEST.test_img is not None:\n",
    "            self.test_img(self.config.TEST.test_img, self.config.TEST.save_dir)\n",
    "            exit()\n",
    "        elif self.config.TEST.test_dir is not None:\n",
    "            self.test_dir(self.config.TEST.test_dir)\n",
    "        else:\n",
    "            psnr, ssim = self._test_dataloader_process(     dataloader    = self.test_dataloader,\n",
    "                                                            scale         = 1.  if not 'scale' in self.config.TEST else self.config.TEST.scale,\n",
    "                                                            add_con       = 0.  if not 'add_con' in self.config.TEST else self.config.TEST.add_con,\n",
    "                                                            floor         = False if not 'floor' in self.config.TEST else self.config.TEST.floor,   \n",
    "                                                            using_bits    = None if not 'using_bits' in self.config.TEST else self.config.TEST.using_bits,\n",
    "                                                            img_save_path = img_save_path,\n",
    "                                                            img_save      = self.config.TEST.save_image)\n",
    "        if psnr is not None and ssim is not None:\n",
    "            with open(os.path.join(self.file_manager.get_dir(img_save_path), '_psnr-%.2f_ssim-%.3f.result'%(psnr, ssim)), 'w') as f:\n",
    "                f.write('PSNR: %f\\nSSIM: %f'%(psnr, ssim))\n",
    "    \n",
    "    def validation(self):\n",
    "        # set denoiser\n",
    "        self._set_main_module()\n",
    "\n",
    "        # make directories for image saving\n",
    "        img_save_path = 'img/val_%03d' % self.epoch\n",
    "        self.file_manager.make_dir(img_save_path)\n",
    "        \n",
    "        psnr, ssim = self._test_dataloader_process(     dataloader    = self.val_dataloader,\n",
    "                                                        scale         = 1.  if not 'scale' in self.config.TEST else self.config.TEST.scale,\n",
    "                                                        add_con       = 0.  if not 'add_con' in self.config.VALIDATION else self.config.VALIDATION.add_con,\n",
    "                                                        floor         = False if not 'floor' in self.config.VALIDATION else self.config.VALIDATION.floor,  \n",
    "                                                        using_bits    = None if not 'using_bits' in self.config.VALIDATION else self.config.VALIDATION.using_bits, \n",
    "                                                        img_save_path = img_save_path,\n",
    "                                                        img_save      = self.config.VALIDATION.save_image)\n",
    "        if psnr is not None and ssim is not None:\n",
    "            with open(os.path.join(self.file_manager.get_dir(img_save_path), '_psnr-%.2f_ssim-%.3f.result'%(psnr, ssim)), 'w') as f:\n",
    "                f.write('PSNR: %f\\nSSIM: %f'%(psnr, ssim))\n",
    "\n",
    "    def _set_module(self):\n",
    "        kwargs = None\n",
    "        if self.config.BASE.model.lower() == 'dncnnflowgan':\n",
    "            kwargs = self.config.MODEL.DNCNNFLOWGAN\n",
    "            kwargs['kwargs_dncnn'] = self.config.MODEL.DNCNN\n",
    "            kwargs['kwargs_flow'] = self.config.MODEL.NMFLOW\n",
    "            kwargs['kwargs_unet'] = self.config.MODEL.UNET\n",
    "        else:\n",
    "            assert False, f\"Invalid model: {self.config.BASE.model}\"\n",
    "        module = {}\n",
    "        if kwargs is None:\n",
    "            module['denoiser'] = get_model_class(self.config.BASE.model)()\n",
    "        else:   \n",
    "            module['denoiser'] = get_model_class(self.config.BASE.model)(**kwargs)\n",
    "        return module\n",
    "    \n",
    "    def _set_optimizer(self):\n",
    "        optimizer = {}\n",
    "        for key in self.module:\n",
    "            optimizer[key] = self._set_one_optimizer(opt        = self.config.OPTIMIZER, \n",
    "                                                     parameters = self.module[key].parameters(), \n",
    "                                                     lr         = float(self.config.TRAIN.init_lr))\n",
    "        return optimizer\n",
    "    \n",
    "    def _forward_fn(self, module, loss, data):\n",
    "        input_data = [data['dataset'][arg] for arg in self.config.MODEL.input_type]\n",
    "        denoised_img = module['denoiser'](*input_data)\n",
    "        model_output = {'recon':denoised_img}\n",
    "        if hasattr(module['denoiser'], 'num_bits'): model_output['num_bits'] = module['denoiser'].num_bits\n",
    "\n",
    "        losses, tmp_info = loss(input_data, model_output, data['dataset'], module, \\\n",
    "                                    ratio=1.)\n",
    "        \n",
    "        return losses, tmp_info\n",
    "    \n",
    "    def _test_dataloader_process(self, dataloader, scale=1., add_con=0., floor=False, using_bits=None, img_save=True, img_save_path=None, info=True):\n",
    "        # make directory\n",
    "        self.file_manager.make_dir(img_save_path)\n",
    "        psnr_avg = AverageMeter()\n",
    "        ssim_avg = AverageMeter()\n",
    "        for idx, data in enumerate(dataloader['dataset']):\n",
    "            # to device\n",
    "            if len(self.device) > 0:\n",
    "                for key in data:\n",
    "                    data[key] = data[key].cuda()\n",
    "                    \n",
    "            input_data = [data[arg] for arg in self.config.MODEL.test_input_type]\n",
    "            if using_bits is not None: input_data.append({'num_bits':using_bits})\n",
    "            denoised_image = self.denoiser(*input_data).detach()        \n",
    "\n",
    "            # add constant and floor (if floor is on)\n",
    "            if scale: denoised_image *= scale\n",
    "            if add_con: denoised_image += add_con\n",
    "            if floor: denoised_image = torch.floor(denoised_image)\n",
    "\n",
    "            # evaluation\n",
    "            if 'clean' in data:\n",
    "                psnr_value = psnr(denoised_image, data['clean'], \n",
    "                                  max_val = 2**using_bits-1 if using_bits else 255)\n",
    "                ssim_value = ssim(denoised_image, data['clean'], \n",
    "                                  data_range= 2**using_bits-1 if using_bits else 255)\n",
    "\n",
    "                psnr_avg.update(psnr_value)\n",
    "                ssim_avg.update(ssim_value)\n",
    "\n",
    "            # image save\n",
    "            if img_save:\n",
    "                # to cpu\n",
    "                if 'clean' in data:\n",
    "                    clean_img = data['clean'].squeeze(0).cpu()\n",
    "                if 'real_noisy' in self.config.MODEL.test_input_type: noisy_img = data['real_noisy']\n",
    "                elif 'syn_noisy' in self.config.MODEL.test_input_type: noisy_img = data['syn_noisy']\n",
    "                elif 'noisy' in self.config.MODEL.test_input_type: noisy_img = data['noisy']\n",
    "                else: noisy_img = None\n",
    "                if noisy_img is not None: noisy_img = noisy_img.squeeze(0).cpu()\n",
    "                denoi_img = denoised_image.squeeze(0).cpu()\n",
    "\n",
    "                # write psnr value on file name\n",
    "                denoi_name = '%05d_DN_%.2f'%(idx, psnr_value) if 'clean' in data else '%05d_DN'%idx\n",
    "\n",
    "                # imwrite\n",
    "                if 'clean' in data:         self.file_manager.save_img_tensor(img_save_path, '%05d_CL'%idx, clean_img, ext=self.config.TEST.save_ext)\n",
    "                if noisy_img is not None: self.file_manager.save_img_tensor(img_save_path, '%05d_N'%idx, noisy_img, ext=self.config.TEST.save_ext)\n",
    "                self.file_manager.save_img_tensor(img_save_path, denoi_name, denoi_img, ext=self.config.TEST.save_ext)\n",
    "                \n",
    "            if info:\n",
    "                if 'clean' in data:\n",
    "                    self.logger.note('[%s] testing... %05d/%05d. PSNR : %.2f dB'%(self.status, idx, dataloader['dataset'].__len__(), psnr_value), end='\\r')\n",
    "                else:\n",
    "                    self.logger.note('[%s] testing... %05d/%05d.'%(self.status, idx, dataloader['dataset'].__len__()), end='\\r')\n",
    "                    \n",
    "        # final log msg\n",
    "        if psnr_avg.count > 0:\n",
    "            self.logger.val('[%s] Done! PSNR : %.2f dB, SSIM : %.3f'%(self.status, psnr_avg.avg, ssim_avg.avg))\n",
    "        else:\n",
    "            self.logger.val('[%s] Done!'%self.status)\n",
    "            \n",
    "        return psnr_avg.avg, ssim_avg.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "n2m",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
