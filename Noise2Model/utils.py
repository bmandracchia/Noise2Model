# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/09_utils.ipynb.

# %% auto 0
__all__ = ['attributesFromDict', 'batch_PSNR', 'compute_index', 'compute_one_hot']

# %% ../nbs/09_utils.ipynb 3
from fastai.vision.all import torch, nn
import numpy as np
from torchmetrics.functional.image import structural_similarity_index_measure as structural_similarity
from torchmetrics.functional.image import peak_signal_noise_ratio
import torch 
import torch.nn.functional as F

# %% ../nbs/09_utils.ipynb 4
def attributesFromDict(d):
    self = d.pop('self')
    for n, v in d.items():
        setattr(self, n, v)

# %% ../nbs/09_utils.ipynb 5
def batch_PSNR(img, imclean, data_range):
    Img = img.data.cpu().numpy().astype(np.float32)
    Iclean = imclean.data.cpu().numpy().astype(np.float32)
    PSNR = 0
    for i in range(Img.shape[0]):
        PSNR += peak_signal_noise_ratio(Iclean[i,:,:,:], Img[i,:,:,:], data_range=data_range)
    return (PSNR/Img.shape[0])

# %% ../nbs/09_utils.ipynb 6
class compute_index():
    def __init__(self, codes, device='cpu') -> None:
        attributesFromDict(locals( ))
      
    def _compute_index(self, b, **kwargs):
        idx = torch.zeros([b], device=self.device, dtype=torch.float32)
        for key, value in self.codes.items():
            idx = idx * len(value)
            for i, v in enumerate(value):
                idx += torch.where(kwargs[key] == v, i, 0.0)

        return idx
    
    def __call__(self, b, **kwargs):
        return self._compute_index(b, **kwargs)

# %% ../nbs/09_utils.ipynb 8
class compute_one_hot():
    def __init__(self, codes, device='cpu') -> None:
        attributesFromDict(locals( ))
      
    def _compute_one_hot(self, b, **kwargs):
        embedding = torch.tensor([])
        for key, value in self.codes.items():
            idx = torch.zeros([b], device=self.device, dtype=torch.float32)
            for i, v in enumerate(value):
                idx += torch.where(kwargs[key] == v, i, 0.0)
            idx_one_hot = F.one_hot(idx.to(torch.int64), num_classes=value.shape[0]).to(torch.float32)
            print(key, ': ', idx_one_hot)
            embedding = torch.cat((embedding, idx_one_hot), dim=1)

        return embedding
    
    def __call__(self, b, **kwargs):
        return self._compute_one_hot(b, **kwargs)
