{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise2NoiseFlow\n",
    "\n",
    "> noiseflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp noise2noiseflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from IPython.display import clear_output, DisplayHandle\n",
    "\n",
    "def update_patch(self, obj):\n",
    "    clear_output(wait=True)\n",
    "    self.display(obj)\n",
    "DisplayHandle.update = update_patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from fastai.vision.all import nn, torch, np\n",
    "from Noise2Model.utils import attributesFromDict\n",
    "from Noise2Model.models import DnCNN, UNet\n",
    "from Noise2Model.utils import gaussian_diag, batch_PSNR, weights_init_orthogonal #, weights_init_kaiming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from Noise2Model.layers.conv2d1x1 import Conv2d1x1\n",
    "from Noise2Model.layers.affine_coupling import AffineCoupling, ShiftAndLogScale\n",
    "from Noise2Model.layers.signal_dependant import SignalDependant\n",
    "from Noise2Model.layers.gain import Gain\n",
    "from Noise2Model.layers.utils import SdnModelScale"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class NoiseFlow(nn.Module):\n",
    "\n",
    "    def __init__(self, x_shape, arch, flow_permutation, param_inits, lu_decomp):\n",
    "        super(NoiseFlow, self).__init__()\n",
    "        attributesFromDict(locals( ))\n",
    "        self.model = nn.ModuleList(self.noise_flow_arch(x_shape))\n",
    "\n",
    "    def noise_flow_arch(self, x_shape):\n",
    "        arch_lyrs = self.arch.split('|')  # e.g., unc|sdn|unc|gain|unc\n",
    "        bijectors = []\n",
    "        for i, lyr in enumerate(arch_lyrs):\n",
    "            # is_last_layer = False\n",
    "\n",
    "            if lyr == 'unc':\n",
    "                if self.flow_permutation == 0:\n",
    "                    pass\n",
    "                elif self.flow_permutation == 1:\n",
    "                    print('|-Conv2d1x1')\n",
    "                    bijectors.append(\n",
    "                        Conv2d1x1(\n",
    "                            num_channels=x_shape[0],\n",
    "                            LU_decomposed=self.decomp,\n",
    "                            name='Conv2d_1x1_{}'.format(i)\n",
    "                        )\n",
    "                    )\n",
    "                else:\n",
    "                    print('|-No permutation specified. Not using any.')\n",
    "                    \n",
    "                print('|-AffineCoupling')\n",
    "                bijectors.append(\n",
    "                    AffineCoupling(\n",
    "                        x_shape=x_shape,\n",
    "                        shift_and_log_scale=ShiftAndLogScale,\n",
    "                        name='unc_%d' % i\n",
    "                    )\n",
    "                )\n",
    "            # elif lyr == 'lt':\n",
    "            #     print('|-LinearTransfomation')\n",
    "            #     bijectors.append(\n",
    "            #         LinearTransformation(\n",
    "            #             name='lt_{}'.format(i),\n",
    "            #             device='cuda'\n",
    "            #         )\n",
    "            #     )\n",
    "            elif lyr == 'sdn':\n",
    "                print('|-SignalDependant')\n",
    "                bijectors.append(\n",
    "                    SignalDependant(\n",
    "                        name='sdn_%d' % i,\n",
    "                        scale=SdnModelScale,\n",
    "                        param_inits=self.param_inits\n",
    "                    )\n",
    "                )\n",
    "            elif lyr == 'gain':\n",
    "                print('|-Gain')\n",
    "                bijectors.append(\n",
    "                    Gain(name='gain_%d' % i)\n",
    "                )\n",
    "\n",
    "        return bijectors\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        z = x\n",
    "        objective = torch.zeros(x.shape[0], dtype=torch.float32, device=x.device)\n",
    "        for bijector in self.model:\n",
    "            z, log_abs_det_J_inv = bijector._forward_and_log_det_jacobian(z, **kwargs)\n",
    "            objective += log_abs_det_J_inv\n",
    "\n",
    "            if 'writer' in kwargs.keys():\n",
    "                kwargs['writer'].add_scalar('model/' + bijector.name, torch.mean(log_abs_det_J_inv), kwargs['step'])\n",
    "        return z, objective\n",
    "\n",
    "    def _loss(self, x, **kwargs):\n",
    "        z, objective = self.forward(x, **kwargs)\n",
    "        # base measure\n",
    "        logp, _ = self.prior(\"prior\", x)\n",
    "\n",
    "        log_z = logp(z)\n",
    "        objective += log_z\n",
    "\n",
    "        if 'writer' in kwargs.keys():\n",
    "            kwargs['writer'].add_scalar('model/log_z', torch.mean(log_z), kwargs['step'])\n",
    "            kwargs['writer'].add_scalar('model/z', torch.mean(z), kwargs['step'])\n",
    "        nobj = - objective\n",
    "        # std. dev. of z\n",
    "        # mu_z = torch.mean(x, dim=[1, 2, 3])\n",
    "        var_z = torch.var(x, dim=[1, 2, 3])\n",
    "        sd_z = torch.mean(torch.sqrt(var_z))\n",
    "\n",
    "        return nobj, sd_z\n",
    "\n",
    "    def loss(self, x, **kwargs):\n",
    "        \n",
    "        # if 'writer' in kwargs.keys():\n",
    "        #     batch_average = torch.mean(x, dim=0)\n",
    "        #     kwargs['writer'].add_histogram('real_noise', batch_average, kwargs['step'])\n",
    "        #     kwargs['writer'].add_scalar('real_noise_std', torch.std(batch_average), kwargs['step'])\n",
    "\n",
    "        nll, sd_z = self._loss(x=x, **kwargs)\n",
    "        nll_dim = torch.mean(nll) / np.prod(x.shape[1:])\n",
    "        # nll_dim = torch.mean(nll)      # The above line should be uncommented\n",
    "\n",
    "        return nll_dim, sd_z\n",
    "\n",
    "    def inverse(self, z, **kwargs):\n",
    "        x = z\n",
    "        for bijector in reversed(self.model):\n",
    "            x = bijector._inverse(x, **kwargs)\n",
    "        return x\n",
    "    \n",
    "    def sample(self, eps_std=None, **kwargs):\n",
    "        _, sample = self.prior(\"prior\", kwargs['clean'])\n",
    "        z = sample(eps_std)\n",
    "        x = self.inverse(z, **kwargs)\n",
    "        batch_average = torch.mean(x, dim=0)\n",
    "        if 'writer' in kwargs.keys():\n",
    "            kwargs['writer'].add_histogram('sample_noise', batch_average, kwargs['step'])\n",
    "            kwargs['writer'].add_scalar('sample_noise_std', torch.std(batch_average), kwargs['step'])\n",
    "\n",
    "        return x\n",
    "\n",
    "    def prior(self, name, x):\n",
    "        n_z = x.shape[1]\n",
    "        h = torch.zeros([x.shape[0]] +  [2 * n_z] + list(x.shape[2:4]), device=x.device)\n",
    "        pz = gaussian_diag(h[:, :n_z, :, :], h[:, n_z:, :, :])\n",
    "\n",
    "        def logp(z1):\n",
    "            objective = pz.logp(z1)\n",
    "            return objective\n",
    "\n",
    "        def sample(eps_std=None):\n",
    "            if eps_std is not None:\n",
    "                z = pz.sample2(pz.eps * torch.reshape(eps_std, [-1, 1, 1, 1]))\n",
    "            else:\n",
    "                z = pz.sample\n",
    "            return z\n",
    "\n",
    "        return logp, sample"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise2NoiseFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class Noise2NoiseFlow(nn.Module):\n",
    "    def __init__(self, x_shape, arch, flow_permutation, param_inits, lu_decomp, denoiser_model='dncnn', dncnn_num_layers=9, lmbda=262144):\n",
    "        super(Noise2NoiseFlow, self).__init__()\n",
    "\n",
    "        self.noise_flow = NoiseFlow(x_shape, arch, flow_permutation, param_inits, lu_decomp)\n",
    "        if denoiser_model == 'dncnn':\n",
    "            self.denoiser = DnCNN(x_shape[0], dncnn_num_layers)\n",
    "            # TODO: self.dncnn should be named self.denoiser by definition, but I changed it here since i needed it to be backward compatible for loading previous models for sampling.\n",
    "            # self.denoiser.apply(weights_init_kaiming)\n",
    "            self.denoiser.apply(weights_init_orthogonal)\n",
    "        elif denoiser_model == 'unet':\n",
    "            self.denoiser = UNet(in_channels=4, out_channels=4)\n",
    "\n",
    "        self.denoiser_loss = nn.MSELoss(reduction='mean')\n",
    "        self.lmbda = lmbda\n",
    "\n",
    "    def denoise(self, noisy, clip=True):\n",
    "        denoised = self.denoiser(noisy)\n",
    "        if clip:\n",
    "            denoised = torch.clamp(denoised, 0., 1.)\n",
    "\n",
    "        return denoised\n",
    "\n",
    "    def forward_u(self, noisy, **kwargs):\n",
    "        denoised = self.denoise(noisy)\n",
    "        kwargs.update({'clean' : denoised})\n",
    "        noise = noisy - denoised\n",
    "\n",
    "        z, objective = self.noise_flow.forward(noise, **kwargs)\n",
    "\n",
    "        return z, objective, denoised\n",
    "\n",
    "    def symmetric_loss(self, noisy1, noisy2, **kwargs):\n",
    "        denoised1 = self.denoise(noisy1)\n",
    "        denoised2 = self.denoise(noisy2)\n",
    "        \n",
    "        noise1 = noisy1 - denoised2\n",
    "        noise2 = noisy2 - denoised1\n",
    "\n",
    "        kwargs.update({'clean' : denoised2})\n",
    "        nll1, _ = self.noise_flow.loss(noise1, **kwargs)\n",
    "\n",
    "        kwargs.update({'clean' : denoised1})\n",
    "        nll2, _ = self.noise_flow.loss(noise2, **kwargs)\n",
    "\n",
    "        nll = (nll1 + nll2) / 2\n",
    "        return nll\n",
    "\n",
    "    def symmetric_loss_with_mse(self, noisy1, noisy2, **kwargs):\n",
    "        denoised1 = self.denoise(noisy1, clip=False)\n",
    "        denoised2 = self.denoise(noisy2, clip=False)\n",
    "\n",
    "        mse_loss1 = self.denoiser_loss(denoised1, noisy2)\n",
    "        mse_loss2 = self.denoiser_loss(denoised2, noisy1)\n",
    "\n",
    "        denoised1 = torch.clamp(denoised1, 0., 1.)\n",
    "        denoised2 = torch.clamp(denoised2, 0., 1.)\n",
    "        \n",
    "        noise1 = noisy1 - denoised2\n",
    "        noise2 = noisy2 - denoised1\n",
    "\n",
    "        kwargs.update({'clean' : denoised2})\n",
    "        nll1, _ = self.noise_flow.loss(noise1, **kwargs)\n",
    "\n",
    "        kwargs.update({'clean' : denoised1})\n",
    "        nll2, _ = self.noise_flow.loss(noise2, **kwargs)\n",
    "\n",
    "        nll = (nll1 + nll2) / 2\n",
    "        mse_loss = (mse_loss1 + mse_loss2) / 2\n",
    "\n",
    "        return nll, mse_loss\n",
    "\n",
    "\n",
    "    def _loss_u(self, noisy1, noisy2, **kwargs):\n",
    "        denoised1 = self.denoise(noisy1, clip=False)\n",
    "\n",
    "        mse_loss = self.denoiser_loss(denoised1, noisy2)\n",
    "\n",
    "        denoised1 = torch.clamp(denoised1, 0., 1.)\n",
    "\n",
    "        noise = noisy1 - denoised1\n",
    "        kwargs.update({'clean' : denoised1})\n",
    "        nll, _ = self.noise_flow.loss(noise, **kwargs)\n",
    "\n",
    "        return nll, mse_loss\n",
    "\n",
    "    def loss_u(self, noisy1, noisy2, **kwargs):\n",
    "        # return self.symmetric_loss(noisy1, noisy2, **kwargs), 0, 0\n",
    "\n",
    "        # nll, mse = self._loss_u(noisy1, noisy2, **kwargs)\n",
    "        nll, mse = self.symmetric_loss_with_mse(noisy1, noisy2, **kwargs)\n",
    "\n",
    "        return nll + self.lmbda * mse, nll.item(), mse.item()\n",
    "        # return nll, nll.item(), mse.item()\n",
    "\n",
    "    def forward_s(self, noise, **kwargs):\n",
    "        return self.noise_flow.forward(noise, **kwargs)\n",
    "\n",
    "    def _loss_s(self, x, **kwargs):\n",
    "        return self.noise_flow._loss(x, **kwargs)\n",
    "\n",
    "    def loss_s(self, x, **kwargs):\n",
    "        return self.noise_flow.loss(x, **kwargs)\n",
    "\n",
    "    def mse_loss(self, noisy, clean, **kwargs):\n",
    "        denoised = self.denoise(noisy, clip=False)\n",
    "        mse_loss = self.denoiser_loss(denoised, clean)\n",
    "        psnr = batch_PSNR(denoised, clean, 1.)\n",
    "        return mse_loss.item(), psnr\n",
    "\n",
    "    def sample(self, eps_std=None, **kwargs):\n",
    "        return self.noise_flow.sample(eps_std, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
