{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NoiseFlow Layers: Signal dependent\n",
    "\n",
    "> noiseflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp layers.signal_dependant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "\n",
    "# from IPython.display import clear_output, DisplayHandle\n",
    "# def update_patch(self, obj):\n",
    "#     clear_output(wait=True)\n",
    "#     self.display(obj)\n",
    "# DisplayHandle.update = update_patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bm/miniconda3/envs/n2m/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "from fastai.vision.all import nn, torch, np\n",
    "from Noise2Model.layers.neural_spline import (unconstrained_rational_quadratic_spline, \\\n",
    "                                            rational_quadratic_spline, sum_except_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SignalDependant(nn.Module):\n",
    "    def __init__(self, scale, param_inits=False, name='sdn'):\n",
    "        super(SignalDependant, self).__init__()\n",
    "        self.name = name\n",
    "        self.param_inits = param_inits\n",
    "        self._scale = scale(self.param_inits)\n",
    "\n",
    "    def _inverse(self, z, **kwargs):\n",
    "        scale = self._scale(kwargs['clean'], kwargs['iso'], kwargs['cam'])\n",
    "        x = z * scale\n",
    "        return x\n",
    "\n",
    "    def _forward_and_log_det_jacobian(self, x, **kwargs):\n",
    "        scale = self._scale(kwargs['clean'], kwargs['iso'], kwargs['cam'])\n",
    "\n",
    "        if 'writer' in kwargs.keys():\n",
    "            kwargs['writer'].add_scalar('model/' + self.name + '_scale_mean', torch.mean(scale), kwargs['step'])\n",
    "            kwargs['writer'].add_scalar('model/' + self.name + '_scale_min', torch.min(scale), kwargs['step'])\n",
    "            kwargs['writer'].add_scalar('model/' + self.name + '_scale_max', torch.max(scale), kwargs['step'])\n",
    "\n",
    "        z = x / scale\n",
    "        log_abs_det_J_inv = - torch.sum(torch.log(scale), dim=[1, 2, 3])\n",
    "        return z, log_abs_det_J_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SignalDependantExp2(nn.Module):\n",
    "    def __init__(self, log_scale, gain_scale, param_inits=False, device='cuda', name='sdn'):\n",
    "        super(SignalDependantExp2, self).__init__()\n",
    "        self.name = name\n",
    "        self.param_inits = param_inits\n",
    "        self._log_scale = log_scale(gain_scale, self.param_inits, device=device, name='sdn_layer_gain_scale')\n",
    "\n",
    "    def _inverse(self, z, **kwargs):\n",
    "        log_scale = self._log_scale(kwargs['clean'], kwargs['iso'], kwargs['cam'])\n",
    "        x = z * torch.exp(log_scale)\n",
    "        return x\n",
    "\n",
    "    def _forward_and_log_det_jacobian(self, x, **kwargs):\n",
    "        writer = kwargs['writer'] if 'writer' in kwargs.keys() else None\n",
    "        step = kwargs['step'] if 'step' in kwargs.keys() else None\n",
    "        \n",
    "        log_scale = self._log_scale(kwargs['clean'], kwargs['iso'], kwargs['cam'], writer, step)\n",
    "\n",
    "        if 'writer' in kwargs.keys():\n",
    "            writer.add_scalar('model/' + self.name + '_log_scale_mean', torch.mean(log_scale), step)\n",
    "            writer.add_scalar('model/' + self.name + '_log_scale_min', torch.min(log_scale), step)\n",
    "            writer.add_scalar('model/' + self.name + '_log_scale_max', torch.max(log_scale), step)\n",
    "\n",
    "        z = x / torch.exp(log_scale)\n",
    "        log_abs_det_J_inv = - torch.sum(log_scale, dim=[1, 2, 3])\n",
    "        return z, log_abs_det_J_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "DEFAULT_MIN_BIN_WIDTH = 1e-3\n",
    "DEFAULT_MIN_BIN_HEIGHT = 1e-3\n",
    "DEFAULT_MIN_DERIVATIVE = 1e-3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SignalDependantNS(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        transform_net,\n",
    "        x_shape,\n",
    "        param_inits=False,\n",
    "        num_bins=10,\n",
    "        tails=\"linear\",\n",
    "        tail_bound=1.0,\n",
    "        name='sdn',\n",
    "        device='cuda',\n",
    "        min_bin_width=DEFAULT_MIN_BIN_WIDTH,\n",
    "        min_bin_height=DEFAULT_MIN_BIN_HEIGHT,\n",
    "        min_derivative=DEFAULT_MIN_DERIVATIVE,\n",
    "        ):\n",
    "        super(SignalDependantNS, self).__init__()\n",
    "        self.name = name\n",
    "        self.ic, self.i0, self.i1 = x_shape\n",
    "        self.num_bins = num_bins\n",
    "        self.tails = tails\n",
    "        self.tail_bound = tail_bound\n",
    "\n",
    "        self.min_bin_width = min_bin_width\n",
    "        self.min_bin_height = min_bin_height\n",
    "        self.min_derivative = min_derivative\n",
    "        \n",
    "        # self._transform_net = transform_net(\n",
    "        #     x_shape=x_shape,\n",
    "        #     width=16,\n",
    "        #     num_in=x_shape[0],\n",
    "        #     num_output=x_shape[0] * self._transform_dim_multiplier(),\n",
    "        #     device=device\n",
    "        # )\n",
    "\n",
    "        self._transform_net = transform_net(\n",
    "            x_shape[0],\n",
    "            x_shape[0] * self._transform_dim_multiplier()\n",
    "        )\n",
    "\n",
    "    def _transform_dim_multiplier(self):\n",
    "        if self.tails == \"linear\":\n",
    "            return self.num_bins * 3 - 1\n",
    "        else:\n",
    "            return self.num_bins * 3 + 1\n",
    "\n",
    "    def _inverse(self, z, **kwargs):\n",
    "        b, c, h, w = z.shape\n",
    "        transform_params = self._transform_net(kwargs['clean'])\n",
    "        transform_params = transform_params.reshape(b, c, -1, h, w).permute(\n",
    "                0, 1, 3, 4, 2\n",
    "            )\n",
    "        unnormalized_widths = transform_params[..., : self.num_bins]\n",
    "        unnormalized_heights = transform_params[..., self.num_bins : 2 * self.num_bins]\n",
    "        unnormalized_derivatives = transform_params[..., 2 * self.num_bins :]\n",
    "\n",
    "        if hasattr(self._transform_net, 'width'):\n",
    "            unnormalized_widths /= np.sqrt(self._transform_net.width)\n",
    "            unnormalized_heights /= np.sqrt(self._transform_net.width)\n",
    "        elif hasattr(self._transform_net, 'hidden_channels'):\n",
    "            unnormalized_widths /= np.sqrt(self._transform_net.hidden_channels)\n",
    "            unnormalized_heights /= np.sqrt(self._transform_net.hidden_channels)\n",
    "        else:\n",
    "            warnings.warn('Inputs to the softmax are not scaled down: initialization might be bad.')\n",
    "\n",
    "        if self.tails is None:\n",
    "            spline_fn = rational_quadratic_spline\n",
    "            spline_kwargs = {}\n",
    "        else:\n",
    "            spline_fn = unconstrained_rational_quadratic_spline\n",
    "            spline_kwargs = {\"tails\": self.tails, \"tail_bound\": self.tail_bound}\n",
    "\n",
    "        x, logabsdet = spline_fn(\n",
    "            inputs=z,\n",
    "            unnormalized_widths=unnormalized_widths,\n",
    "            unnormalized_heights=unnormalized_heights,\n",
    "            unnormalized_derivatives=unnormalized_derivatives,\n",
    "            inverse=True,\n",
    "            min_bin_width=self.min_bin_width,\n",
    "            min_bin_height=self.min_bin_height,\n",
    "            min_derivative=self.min_derivative,\n",
    "            **spline_kwargs\n",
    "        )\n",
    "\n",
    "        logabsdet = sum_except_batch(logabsdet)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _forward_and_log_det_jacobian(self, x, **kwargs):\n",
    "        b, c, h, w = x.shape\n",
    "        transform_params = self._transform_net(kwargs['clean'])\n",
    "        transform_params = transform_params.reshape(b, c, -1, h, w).permute(\n",
    "                0, 1, 3, 4, 2\n",
    "            )\n",
    "        unnormalized_widths = transform_params[..., : self.num_bins]\n",
    "        unnormalized_heights = transform_params[..., self.num_bins : 2 * self.num_bins]\n",
    "        unnormalized_derivatives = transform_params[..., 2 * self.num_bins :]\n",
    "\n",
    "        if hasattr(self._transform_net, 'width'):\n",
    "            unnormalized_widths /= np.sqrt(self._transform_net.width)\n",
    "            unnormalized_heights /= np.sqrt(self._transform_net.width)\n",
    "        elif hasattr(self._transform_net, 'hidden_channels'):\n",
    "            unnormalized_widths /= np.sqrt(self._transform_net.hidden_channels)\n",
    "            unnormalized_heights /= np.sqrt(self._transform_net.hidden_channels)\n",
    "        else:\n",
    "            warnings.warn('Inputs to the softmax are not scaled down: initialization might be bad.')\n",
    "\n",
    "        if self.tails is None:\n",
    "            spline_fn = rational_quadratic_spline\n",
    "            spline_kwargs = {}\n",
    "        else:\n",
    "            spline_fn = unconstrained_rational_quadratic_spline\n",
    "            spline_kwargs = {\"tails\": self.tails, \"tail_bound\": self.tail_bound}\n",
    "\n",
    "        z, logabsdet = spline_fn(\n",
    "            inputs=x,\n",
    "            unnormalized_widths=unnormalized_widths,\n",
    "            unnormalized_heights=unnormalized_heights,\n",
    "            unnormalized_derivatives=unnormalized_derivatives,\n",
    "            inverse=False,\n",
    "            min_bin_width=self.min_bin_width,\n",
    "            min_bin_height=self.min_bin_height,\n",
    "            min_derivative=self.min_derivative,\n",
    "            **spline_kwargs\n",
    "        )\n",
    "\n",
    "        logabsdet = sum_except_batch(logabsdet)\n",
    "\n",
    "        return z, logabsdet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
