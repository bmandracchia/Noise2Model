{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss\n",
    "\n",
    "> Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from Noise2Model.utils import StandardNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch import randn as torch_randn\n",
    "from fastai.vision.all import test_eq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "loss_class_dict = {}\n",
    "\n",
    "def regist_loss(loss):\n",
    "    loss_name = loss.__name__.lower()\n",
    "    assert not loss_name in loss_class_dict, 'there is already registered dataset: %s in trainer_dict.' % loss_name\n",
    "    loss_class_dict[loss_name] = loss\n",
    "\n",
    "    return loss\n",
    "\n",
    "def get_loss_class(loss_name:str):\n",
    "    loss_name = loss_name.lower()\n",
    "    return loss_class_dict[loss_name]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class Loss(nn.Module):\n",
    "    def __init__(self, loss_string, tmp_info=[]):\n",
    "        super().__init__()\n",
    "        loss_string     = loss_string.replace(' ', '').lower()\n",
    "\n",
    "        # parse loss string\n",
    "        self.loss_list = []\n",
    "        for single_loss in loss_string.split('+'):\n",
    "            weight, name = single_loss.split('*')\n",
    "            ratio = True if 'r' in weight else False\n",
    "            weight = float(weight.replace('r', ''))\n",
    "\n",
    "            if name in loss_class_dict:\n",
    "                self.loss_list.append({ 'name': name,\n",
    "                                        'weight': float(weight),\n",
    "                                        'func': loss_class_dict[name](),\n",
    "                                        'ratio': ratio})\n",
    "            else:\n",
    "                raise RuntimeError('undefined loss term: {}'.format(name))\n",
    "            \n",
    "        # parse temporal information string\n",
    "        self.tmp_info_list = []\n",
    "        for name in tmp_info:\n",
    "            lname=name.lower()\n",
    "            if lname in loss_class_dict:\n",
    "                self.tmp_info_list.append({ 'name': lname,\n",
    "                                            'func': loss_class_dict[name]()})\n",
    "            else:\n",
    "                raise RuntimeError('undefined loss term: {}'.format(lname))\n",
    "\n",
    "\n",
    "    def forward(self, input_data, model_output, data, module, loss_name=None, change_name=None, ratio=1.0):\n",
    "        '''\n",
    "        forward all loss and return as dict format.\n",
    "        Args\n",
    "            input_data   : input of the network (also in the data)\n",
    "            model_output : output of the network\n",
    "            data         : entire batch of data\n",
    "            module       : dictionary of modules (for another network forward)\n",
    "            loss_name    : (optional) choose specific loss with name\n",
    "            change_name  : (optional) replace name of chosen loss\n",
    "            ratio        : (optional) percentage of learning procedure for increase weight during training\n",
    "        Return\n",
    "            losses       : dictionary of loss\n",
    "        '''\n",
    "        loss_arg = (input_data, model_output, data, module)\n",
    "\n",
    "        # calculate only specific loss 'loss_name' and change its name to 'change_name'\n",
    "        if loss_name is not None:\n",
    "            for single_loss in self.loss_list:\n",
    "                if loss_name == single_loss['name']:\n",
    "                    loss = single_loss['weight'] * single_loss['func'](*loss_arg)\n",
    "                    if single_loss['ratio']: loss *= ratio\n",
    "                    if change_name is not None:\n",
    "                        return {change_name: loss}\n",
    "                    return {single_loss['name']: loss}\n",
    "            raise RuntimeError('there is no such loss in training losses: {}'.format(loss_name))\n",
    "\n",
    "        # normal case: calculate all training losses at one time\n",
    "        losses = {}\n",
    "        for single_loss in self.loss_list:\n",
    "            losses[single_loss['name']] = single_loss['weight'] * single_loss['func'](*loss_arg)\n",
    "            if single_loss['ratio']: losses[single_loss['name']] *= ratio \n",
    "\n",
    "        # calculate temporal information\n",
    "        tmp_info = {}\n",
    "        for single_tmp_info in self.tmp_info_list:\n",
    "            # don't need gradient\n",
    "            with torch.no_grad():\n",
    "                tmp_info[single_tmp_info['name']] = single_tmp_info['func'](*loss_arg)\n",
    "\n",
    "        return losses, tmp_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def _mse(x, y, level=2):\n",
    "    assert x.shape == y.shape\n",
    "    err = (x - y) ** level\n",
    "    err = torch.abs(err)\n",
    "    return err.mean()\n",
    "\n",
    "\n",
    "@regist_loss\n",
    "class L1Loss(nn.Module):\n",
    "    def forward(self, input_data, model_output, data, module):\n",
    "        fx = model_output['recon']\n",
    "        y = data['clean']\n",
    "        return _mse(fx, y, level=1)\n",
    "\n",
    "\n",
    "@regist_loss\n",
    "class L2Loss(nn.Module):\n",
    "    def forward(self, input_data, model_output, data, module):\n",
    "        fx = model_output['recon']\n",
    "        y = data['clean']\n",
    "        return _mse(fx, y, level=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@regist_loss\n",
    "class NLLLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dist = StandardNormal()\n",
    "\n",
    "    def forward(self, input_data, model_output, data, module):\n",
    "        z = model_output['z']\n",
    "        ldj = model_output['ldj']\n",
    "\n",
    "        log_z = self.dist.log_prob(z)\n",
    "        objectives = ldj + log_z\n",
    "        return torch.mean(-objectives)\n",
    "\n",
    "@regist_loss\n",
    "class std_z(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dist = StandardNormal()\n",
    "\n",
    "    def forward(self, input_data, model_output, data, module):\n",
    "        z = model_output['z']\n",
    "        var_z = torch.var(z, dim=[1,2,3])\n",
    "        sd_z = torch.mean(torch.sqrt(var_z))\n",
    "        return sd_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@regist_loss\n",
    "class GANLoss(nn.Module):\n",
    "    def __init__(self, lambda_gp=10., lambda_gen=1.0):\n",
    "        super(GANLoss, self).__init__()\n",
    "        self.lambda_gen = lambda_gen\n",
    "        self.lambda_gp = lambda_gp\n",
    "        \n",
    "    def get_target_tensor(self, prediction, target_is_real):\n",
    "        if target_is_real:\n",
    "            target_tensor = self.real_label\n",
    "        else:\n",
    "            target_tensor = self.fake_label\n",
    "        return target_tensor.expand_as(prediction)\n",
    "\n",
    "    def forward(self, input_data, model_output, data, module):\n",
    "        training_mode = model_output['training_mode']\n",
    "        if training_mode == 'generator':\n",
    "            loss = -model_output['critic_fake'].mean() * self.lambda_gen\n",
    "        elif training_mode == 'critic':\n",
    "            if model_output['critic_noise']:\n",
    "                #REMARKS: If this part is changed, _forward_fn must also be changed.\n",
    "                fake_noise = (model_output['fake']-data['clean']).requires_grad_(True)\n",
    "                real_noise = (model_output['real']-data['clean']).requires_grad_(True)\n",
    "                gp_loss = self._gradient_penalty(\n",
    "                    module['critic'],\n",
    "                    torch.cat([real_noise, data['clean']],dim=1).requires_grad_(True),\n",
    "                    torch.cat([fake_noise, data['clean']],dim=1).requires_grad_(True)\n",
    "                )\n",
    "            else:\n",
    "                gp_loss = self._gradient_penalty(module['critic'], model_output['real'], model_output['fake'])\n",
    "            loss = model_output['critic_fake'].mean() - model_output['critic_real'].mean() \\\n",
    "                + self.lambda_gp * gp_loss\n",
    "        else:\n",
    "            assert False, f'Invalid training mode: {training_mode}'\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _gradient_penalty(self, D, real_samples, fake_samples):\n",
    "        alpha = torch.randn(real_samples.size(0), 1, 1, 1, device=real_samples.device)\n",
    "        interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "        d_interpolates = D(interpolates)\n",
    "        fake = torch.ones([real_samples.shape[0], 1], device=real_samples.device)\n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=d_interpolates,\n",
    "            inputs=interpolates,\n",
    "            grad_outputs=fake,\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True,\n",
    "        )[0]\n",
    "        gradients = gradients.view(gradients.size(0), -1)\n",
    "        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "        return gradient_penalty\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@regist_loss\n",
    "class real_sub_fake(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input_data, model_output, data, module):\n",
    "        return model_output['critic_real'].mean() - model_output['critic_fake'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "n2m",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
