# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/04_noiseflow.ipynb.

# %% auto 0
__all__ = ['NoiseFlow']

# %% ../nbs/04_noiseflow.ipynb 4
from fastai.vision.all import nn, torch, np, Path, get_image_files, Image 
import normflows as nf
from .utils import attributesFromDict
# from Noise2Model.models import DnCNN, UNet
from .utils import gaussian_diag #, batch_PSNR, weights_init_orthogonal #, weights_init_kaiming


# %% ../nbs/04_noiseflow.ipynb 6
from .layers import Unconditional, Gain, AffineSdn

# %% ../nbs/04_noiseflow.ipynb 7
class NoiseFlow(nn.Module):

    def __init__(self, x_shape, arch, device='cuda'): # device may be removed
        super(NoiseFlow, self).__init__()
        attributesFromDict(locals( ))
        self.model = nn.ModuleList(self.noise_flow_arch(x_shape))

    def noise_flow_arch(self, x_shape):
        arch_lyrs = self.arch.split('|')  # e.g., unc|sdn|unc|gain|unc
        bijectors = []
        for i, lyr in enumerate(arch_lyrs):
            # is_last_layer = False

            if lyr == 'unc':
                print('|-AffineCoupling')
                bijectors.append(
                    Unconditional(
                        channels=x_shape[1],
                        hidden_channels = 16,
                        split_mode='channel' if x_shape[1] != 1 else 'checkerboard'
                    )#.to(self.device)
                )
            elif lyr == 'sdn':
                print('|-SignalDependant')
                bijectors.append(
                    AffineSdn(x_shape[1:])#.to(self.device)
                )
            elif lyr == 'gain':
                print('|-Gain')
                bijectors.append(
                    Gain(x_shape[1:])#.to(self.device)    # Gain and offset
                )

        return bijectors

    def _forward(self, x, **kwargs):
        z = x
        objective = torch.zeros(x.shape[0], dtype=torch.float32, device=self.device)
        for bijector in self.model:
            z, log_abs_det_J_inv = bijector.forward(z, **kwargs)
            objective += log_abs_det_J_inv

            if 'writer' in kwargs.keys():
                kwargs['writer'].add_scalar('model/' + bijector.name, torch.mean(log_abs_det_J_inv), kwargs['step'])
        return z, objective
    
    def forward(self, x, **kwargs):
        z = x
        objective = torch.zeros(x.shape[0], dtype=torch.float32, device=self.device)
        for bijector in self.model:
            z, log_abs_det_J_inv = bijector.forward(z, **kwargs)
            objective += log_abs_det_J_inv

            if 'writer' in kwargs.keys():
                kwargs['writer'].add_scalar('model/' + bijector.name, torch.mean(log_abs_det_J_inv), kwargs['step'])
        return z#, objective

    def _loss(self, x, **kwargs):
        z, objective = self._forward(x, **kwargs)
        # base measure
        logp, _ = self.prior("prior", x)

        log_z = logp(z)
        objective += log_z

        if 'writer' in kwargs.keys():
            kwargs['writer'].add_scalar('model/log_z', torch.mean(log_z), kwargs['step'])
            kwargs['writer'].add_scalar('model/z', torch.mean(z), kwargs['step'])
        nobj = - objective
        # std. dev. of z
        # mu_z = torch.mean(x, dim=[1, 2, 3])
        var_z = torch.var(x, dim=[1, 2, 3])
        sd_z = torch.mean(torch.sqrt(var_z))

        return nobj, sd_z

    def loss(self, x, **kwargs):
        
        # if 'writer' in kwargs.keys():
        #     batch_average = torch.mean(x, dim=0)
        #     kwargs['writer'].add_histogram('real_noise', batch_average, kwargs['step'])
        #     kwargs['writer'].add_scalar('real_noise_std', torch.std(batch_average), kwargs['step'])

        nll, sd_z = self._loss(x=x, **kwargs)
        nll_dim = torch.mean(nll) / np.prod(x.shape[1:])
        # nll_dim = torch.mean(nll)      # The above line should be uncommented

        return nll_dim, sd_z

    def inverse(self, z, **kwargs):
        x = z
        for bijector in reversed(self.model):
            x = bijector._inverse(x, **kwargs)
        return x
    
    def sample(self, eps_std=None, **kwargs):
        _, sample = self.prior("prior", kwargs['clean'])
        z = sample(eps_std)
        x = self.inverse(z, **kwargs)
        batch_average = torch.mean(x, dim=0)
        if 'writer' in kwargs.keys():
            kwargs['writer'].add_histogram('sample_noise', batch_average, kwargs['step'])
            kwargs['writer'].add_scalar('sample_noise_std', torch.std(batch_average), kwargs['step'])

        return x

    def prior(self, name, x):
        n_z = x.shape[1]
        h = torch.zeros([x.shape[0]] +  [2 * n_z] + list(x.shape[2:4]), device=x.device)
        pz = gaussian_diag(h[:, :n_z, :, :], h[:, n_z:, :, :])

        def logp(z1):
            objective = pz.logp(z1)
            return objective

        def sample(eps_std=None):
            if eps_std is not None:
                z = pz.sample2(pz.eps * torch.reshape(eps_std, [-1, 1, 1, 1]))
            else:
                z = pz.sample
            return z

        return logp, sample
