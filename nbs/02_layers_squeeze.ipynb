{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core\n",
    "\n",
    "> Core functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp layers.squeeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastai.vision.all import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import randn as torch_randn\n",
    "from fastai.vision.all import test_eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def squeeze_spatial(input, factor=2):\n",
    "    assert factor >= 1 and isinstance(factor, int), \"Factor must be a positive integer.\"\n",
    "    \n",
    "    if factor == 1: return input\n",
    "    \n",
    "    dim = input.dim()\n",
    "    assert dim >= 3 and dim <= 5, \"Input tensor must have 3D, 4D, or 5D dimensions.\"\n",
    "    \n",
    "    sizes = list(input.size())\n",
    "    batch_size, channels = sizes[:2]\n",
    "    spatial_dims = sizes[2:]\n",
    "    \n",
    "    for dim_idx in range(len(spatial_dims)):\n",
    "        assert spatial_dims[dim_idx] % factor == 0, \"Spatial dimension {} is not divisible by the factor.\".format(dim_idx)\n",
    "        sizes[dim_idx + 2] //= factor\n",
    "    \n",
    "    input = input.view(batch_size, channels, *spatial_dims)  # Ensure contiguous memory\n",
    "    \n",
    "    new_channels = channels * (factor ** (dim_idx + 1))\n",
    "    new_spatial_dims = sizes[2:]\n",
    "\n",
    "    return  input.view(batch_size, new_channels, *new_spatial_dims)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch_randn(16,1,64,64)\n",
    "test_eq(squeeze_spatial(x).shape, [16,4,32,32])\n",
    "\n",
    "x = torch_randn(16,1,64,64,32)\n",
    "test_eq(squeeze_spatial(x).shape, [16,8,32,32,16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def unsqueeze_spatial(input, factor=2):\n",
    "    assert factor >= 1 and isinstance(factor, int), \"Factor must be a positive integer.\"\n",
    "    \n",
    "    if factor == 1: return input\n",
    "    \n",
    "    dim = input.dim()\n",
    "    assert dim >= 3 and dim <= 5, \"Input tensor must have 3D, 4D, or 5D dimensions.\"\n",
    "    \n",
    "    sizes = list(input.size())\n",
    "    batch_size, channels = sizes[:2]\n",
    "    spatial_dims = sizes[2:]\n",
    "    \n",
    "    sizes = [sizes[0], sizes[1]]\n",
    "    for dim_idx in range(len(spatial_dims)):\n",
    "        sizes.append(spatial_dims[dim_idx] * factor)\n",
    "            \n",
    "    input = input.view(batch_size, channels, *spatial_dims)  # Ensure contiguous memory\n",
    "    \n",
    "    new_channels = channels // (factor ** (len(spatial_dims)))\n",
    "    new_spatial_dims = sizes[2:]\n",
    "    \n",
    "    return  input.view(batch_size, new_channels, *new_spatial_dims)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch_randn(16,4,32,32)\n",
    "test_eq(unsqueeze_spatial(x).shape, [16,1,64,64])\n",
    "\n",
    "x = torch_randn(16,8,32,32,16)\n",
    "test_eq(unsqueeze_spatial(x).shape, [16,1,64,64,32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SqueezeLayer(nn.Module):\n",
    "\tdef __init__(self, factor, level, name='squeeze'):\n",
    "\t\tsuper(SqueezeLayer, self).__init__()\n",
    "\t\tself.factor = factor\n",
    "\t\tself.name = name\n",
    "\t\tself.level = level\n",
    "\n",
    "\tdef _inverse(self, z, **kwargs):\n",
    "\t\toutput = unsqueeze_spatial(z, self.factor)\n",
    "\t\treturn output\n",
    "\n",
    "\tdef _forward_and_log_det_jacobian(self, x, **kwargs):\n",
    "\t\toutput = squeeze_spatial(x, self.factor)\n",
    "\t\treturn output, 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch_randn(16,1,64,64)\n",
    "\n",
    "tst = SqueezeLayer(2, 1)\n",
    "z, _ = tst._forward_and_log_det_jacobian(x)\n",
    "test_eq(z.shape, [16,4,32,32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "n2m",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
