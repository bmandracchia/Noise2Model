# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_models.ipynb.

# %% auto 0
__all__ = ['DnCNN', 'SubNetConv', 'UNet']

# %% ../nbs/01_models.ipynb 4
from fastai.vision.all import ConvLayer, Lambda, MaxPool, NormType, nn, np
from torch import cat as torch_cat
import torch.nn.functional as F
from .utils import attributesFromDict

# %% ../nbs/01_models.ipynb 7
class DnCNN(nn.Module):
    def __init__(self, channels, num_of_layers=9, features=64, kernel_size=3):
        super(DnCNN, self).__init__()
        padding = 1
        layers = []
        layers.append(ConvLayer(channels, features,
                      ks=kernel_size, padding=padding, norm_type=None))
        for _ in range(num_of_layers-2):
            layers.append(ConvLayer(features, features,
                          ks=kernel_size, padding=padding))
        layers.append(nn.Conv2d(in_channels=features, out_channels=channels,
                      kernel_size=kernel_size, padding=padding, bias=False))
        self.dncnn = nn.Sequential(*layers)

    def forward(self, x):
        residual = self.dncnn(x)
        denoised = x - residual
        return denoised

# %% ../nbs/01_models.ipynb 9
def SubNetConv(ks=3,
               stride=1,
               padding=None,
               bias=None,
               ndim=2,
               norm_type=NormType.Batch,
               bn_1st=True,
               act_cls=nn.ReLU,
               transpose=False,
               init='auto',
               xtra=None,
               bias_std=0.01,
               dropout=0.0,
               ):

    def _conv(n_in, n_out, n_conv=1):
        s = ConvLayer(n_in, n_out, ks=ks, stride=stride, padding=padding, bias=bias, ndim=ndim, norm_type=norm_type, bn_1st=bn_1st,
                      act_cls=act_cls, transpose=transpose, init=init, xtra=xtra, bias_std=bias_std)
        if dropout is not None and dropout > 0:
            s = nn.Sequential(s, nn.Dropout(dropout))
        for _ in range(n_conv-1):
            t = ConvLayer(n_out, n_out, ks=ks, stride=stride, padding=padding, bias=bias, ndim=ndim, norm_type=norm_type, bn_1st=bn_1st,
                          act_cls=act_cls, transpose=transpose, init=init, xtra=xtra, bias_std=bias_std)
            if dropout is not None and dropout > 0:
                t = nn.Sequential(t, nn.Dropout(dropout))
            s = nn.Sequential(s, t)
        return s

    return _conv

# %% ../nbs/01_models.ipynb 11
class _Net_recurse(nn.Module):
    def __init__(self,
                 depth=4,						# depth of the UNet network
                 mult_chan=32,					# number of filters at first layer
                 in_channels=1,					# number of input channels
                 kernel_size=3,					# kernel size of convolutional layers
                 ndim=2,							# number of spatial dimensions of the input data
                 n_conv_per_depth=2,				# number of convolutions per layer
                 activation=nn.ReLU,				# activation function used in convolutional layers
                 norm_type=NormType.Batch,
                 dropout=0.0,
                 pool=MaxPool,
                 pool_size=2,
                 ):
        """Class for recursive definition of U-network.p

        Parameters:
        in_channels - (int) number of channels for input.
        mult_chan - (int) factor to determine number of output channels
        depth - (int) if 0, this subnet will only be convolutions that double the channel count.
        """
        super().__init__()
        # Parameters
        self.depth = depth
        n_out = in_channels*mult_chan

        # Layer types
        Pooling = pool(ks=pool_size, ndim=ndim)
        UpSample = nn.Upsample(scale_factor=pool_size, mode='nearest')
        SubNet_Conv = SubNetConv(ks=kernel_size, stride=1, padding=None, bias=None, ndim=ndim, norm_type=norm_type,
                                 bn_1st=True, act_cls=activation, transpose=False, dropout=dropout)

        # Blocks
        self.sub_conv_more = SubNet_Conv(in_channels, n_out, n_conv_per_depth)
        if self.depth > 0:
            in_channels = n_out
            mult_chan = 2
            depth = (self.depth - 1)
            self.sub_u = nn.Sequential(Pooling,                                                         # layer reducing the image size (usually a pooling layer)
                                       _Net_recurse(depth, mult_chan, in_channels, kernel_size,
                                                    ndim, n_conv_per_depth, activation, norm_type,
                                                    dropout, pool, pool_size),                          # lower unet level
                                       # layer increasing the image size (usually an upsampling layer)
                                       UpSample,
                                       )
            self.sub_conv_less = SubNet_Conv(3*n_out, n_out, n_conv_per_depth)

    def forward(self, x):
        if self.depth == 0:
            return self.sub_conv_more(x)
        else:  # depth > 0
            # convolutions with increasing number of channels
            x_conv_more = self.sub_conv_more(x)
            x_from_sub_u = self.sub_u(x_conv_more)
            # concatenate the upsampled outputs of the lower level with the outputs of the next level in size
            x_cat = torch_cat((x_from_sub_u, x_conv_more), 1)
            # convolutions with decreasing number of channels
            x_conv_less = self.sub_conv_less(x_cat)
        return x_conv_less

# %% ../nbs/01_models.ipynb 12
class UNet(nn.Module):
    def __init__(self,
                 depth=4,						# depth of the UNet network
                 mult_chan=32,					# number of filters at first layer
                 in_channels=1,					# number of input channels
                 out_channels=1,					# number of output channels
                 last_activation=None,			# last activation before final result
                 kernel_size=3,					# kernel size of convolutional layers
                 ndim=2,							# number of spatial dimensions of the input data
                 n_conv_per_depth=2,				# number of convolutions per layer
                 activation='ReLU',				# activation function used in convolutional layers
                 norm_type=NormType.Batch,
                 dropout=0.0,
                 pool=MaxPool,
                 pool_size=2,
                 residual=False,
                 prob_out=False,
                 eps_scale=1e-3,
                 ):
        super().__init__()
        last_activation = getattr(F, f"{activation.lower()}") if last_activation == None else getattr(
            F, f"{last_activation.lower()}")
        activation = getattr(nn, f"{activation}")
        attributesFromDict(locals())		# stores all the input parameters in self

        self.net_recurse = _Net_recurse(depth, mult_chan, in_channels, kernel_size, ndim,
                                        n_conv_per_depth, activation, norm_type, dropout, pool, pool_size)
        self.conv_out = ConvLayer(mult_chan, out_channels, ndim=ndim,
                                  ks=kernel_size, norm_type=None, act_cls=None, padding=1)

    def forward(self, x):
        x_rec = self.net_recurse(x)
        final = self.conv_out(x_rec)

        if self.residual:
            if not (self.out_channels == self.in_channels):
                raise ValueError(
                    "number of input and output channels must be the same for a residual net.")
            final = final + x
        final = self.last_activation(final)

        if self.prob_out:
            scale = ConvLayer(self.out_channels, self.out_channels,
                              ndim=self.ndim, ks=1, norm_type=None, act_cls=nn.Softplus)(x_rec)
            scale = Lambda(lambda x: x+np.float32(self.eps_scale))(scale)
            final = torch_cat((final, scale), 1)

        return final
