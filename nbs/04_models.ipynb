{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "> models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from typing import Iterator\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "from core.model import regist_model, get_model_class\n",
    "from core.model.flow_layers import get_flow_layer\n",
    "from util.standard_normal_dist import StandardNormal\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise Modeler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@regist_model\n",
    "class NMFlow(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_ch=1,\n",
    "        ch_exp_coef = 1.,\n",
    "        width_exp_coef = 2.,\n",
    "        num_bits=16,\n",
    "        conv_net_feats=16,\n",
    "        pre_arch=\"UD\",\n",
    "        arch=\"NE|SAL|SDL|CL2|SAL|SDL|CL2\"\n",
    "    ):\n",
    "        super(NMFlow, self).__init__()\n",
    "        self.num_bits=num_bits\n",
    "\n",
    "        self.in_ch = in_ch\n",
    "        self.ch_exp_coef = ch_exp_coef\n",
    "        self.width_exp_coef = width_exp_coef\n",
    "        self.conv_net_feats = conv_net_feats\n",
    "\n",
    "        self.pre_bijectors = list()\n",
    "        pre_arch_lyrs = pre_arch.split('|')\n",
    "        for lyr in pre_arch_lyrs:\n",
    "            self.pre_bijectors.append(self.get_flow_layer(lyr))\n",
    "        self.pre_bijectors = nn.Sequential(*self.pre_bijectors)\n",
    "\n",
    "        self.bijectors = list()\n",
    "        arch_lyrs = arch.split('|')\n",
    "        for lyr in arch_lyrs:\n",
    "            self.bijectors.append(self.get_flow_layer(lyr))\n",
    "        self.bijectors = nn.Sequential(*self.bijectors)\n",
    "        self.dist = StandardNormal()\n",
    "\n",
    "    def internal_channels(self):\n",
    "        return int(self.in_ch * self.ch_exp_coef)\n",
    "    \n",
    "    def internal_widths(self):\n",
    "        return int(self.in_ch * self.width_exp_coef)\n",
    "\n",
    "    def get_flow_layer(self, name):\n",
    "        if name == \"UD\":\n",
    "            return get_flow_layer(\"UniformDequantization\")(device='cuda', num_bits=self.num_bits)\n",
    "        elif name == \"NE\":\n",
    "            return get_flow_layer(\"NoiseExtraction\")(device='cuda')\n",
    "        elif name == \"CL2\":\n",
    "            return get_flow_layer(\"ConditionalLinearExp2\")(\n",
    "                in_ch=self.internal_channels(),\n",
    "                device='cuda'\n",
    "            )\n",
    "        elif name == \"SDL\":\n",
    "            return get_flow_layer(\"SignalDependentConditionalLinear\")(\n",
    "                meta_encoder=lambda in_features, out_features: get_flow_layer(\"ResidualNet\")(\n",
    "                    in_features=in_features,\n",
    "                    out_features=out_features,\n",
    "                    hidden_features=5,\n",
    "                    num_blocks=3,\n",
    "                    use_batch_norm=True,\n",
    "                    dropout_probability=0.0\n",
    "                ),\n",
    "                scale_and_bias=lambda in_features, out_features: get_flow_layer(\"PointwiseConvs\")(\n",
    "                    in_features=in_features,\n",
    "                    out_features=out_features,\n",
    "                    feats=self.conv_net_feats\n",
    "                ),\n",
    "                in_ch=self.internal_channels(),\n",
    "                device='cuda'\n",
    "            )\n",
    "        elif name == \"SAL\":\n",
    "            return get_flow_layer(\"StructureAwareConditionalLinearLayer\")(\n",
    "                meta_encoder=lambda in_features, out_features: get_flow_layer(\"ResidualNet\")(\n",
    "                    in_features=in_features,\n",
    "                    out_features=out_features,\n",
    "                    hidden_features=5,\n",
    "                    num_blocks=3,\n",
    "                    use_batch_norm=True,\n",
    "                    dropout_probability=0.0\n",
    "                ),\n",
    "                structure_encoder=lambda in_features, out_features: get_flow_layer(\"SpatialConvs\")(\n",
    "                    in_features=in_features,\n",
    "                    out_features=out_features,\n",
    "                    receptive_field=9,\n",
    "                    feats=self.conv_net_feats\n",
    "                ),\n",
    "                in_ch=self.internal_channels(),\n",
    "                device='cuda'\n",
    "            )\n",
    "        else:\n",
    "            assert False, f\"Invalid layer name : {name}\"\n",
    "\n",
    "    def forward(self, noisy, clean, kwargs=dict()):\n",
    "        x = noisy\n",
    "        kwargs['clean'] = clean.clone()\n",
    "\n",
    "        objectives = 0.\n",
    "        for bijector in self.pre_bijectors:\n",
    "            if isinstance(bijector, get_flow_layer(\"UniformDequantization\")):\n",
    "                kwargs['clean'], _ = bijector._forward_and_log_det_jacobian(kwargs['clean'])\n",
    "\n",
    "            x, ldj = bijector._forward_and_log_det_jacobian(x, **kwargs)\n",
    "            objectives += ldj\n",
    "\n",
    "        for bijector in self.bijectors:\n",
    "            x, ldj = bijector._forward_and_log_det_jacobian(x, **kwargs)\n",
    "            objectives += ldj\n",
    "        return x, objectives\n",
    "\n",
    "    def sample(self, kwargs=dict()):\n",
    "        for bijector in self.pre_bijectors:\n",
    "            if isinstance(bijector, get_flow_layer(\"UniformDequantization\")):\n",
    "                kwargs['clean'], _ = bijector._forward_and_log_det_jacobian(kwargs['clean'], **kwargs)\n",
    "\n",
    "        b,_,h,w = kwargs['clean'].shape\n",
    "        x = self.dist.sample((b,self.internal_channels(),h,w))\n",
    "        for bijector in reversed(self.bijectors):\n",
    "            x = bijector._inverse(x, **kwargs)\n",
    "\n",
    "        for bijector in reversed(self.pre_bijectors):\n",
    "            if isinstance(bijector, get_flow_layer(\"UniformDequantization\")):\n",
    "                kwargs['clean'] = bijector._inverse(kwargs['clean'], **kwargs)\n",
    "            x = bijector._inverse(x, **kwargs)\n",
    "        x = torch.clip(x, 0, 2**self.num_bits)\n",
    "        return x \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Denoiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class NMFlowDenoiser(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            denoiser,\n",
    "            kwargs_flow,\n",
    "            flow_pth_path,\n",
    "            num_bits=14,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.denoiser = denoiser\n",
    "        self.kwargs_flow = kwargs_flow\n",
    "        self.flow_pth_path = flow_pth_path\n",
    "        self.num_bits = num_bits\n",
    "        self.noise_model = get_model_class(\"NMFlow\")(**kwargs_flow)\n",
    "        self._load_checkpoint(self.noise_model, flow_pth_path)\n",
    "\n",
    "    def _load_checkpoint(self, module, path, name='noise_model'):\n",
    "        assert os.path.exists(path), f\"{path} is not exist.\"\n",
    "        pth = torch.load(path)\n",
    "        module.load_state_dict(pth['model_weight'][name])\n",
    "        module.eval()\n",
    "    \n",
    "    def parameters(self, recurse: bool = True) -> Iterator[Parameter]:\n",
    "        return self.denoiser.parameters(recurse) # the parameters of denoiser will be trained only.\n",
    "        \n",
    "    def forward(self, x, kwargs=dict()):\n",
    "        # x: clean image\n",
    "        x_scaled = x / (2**self.num_bits) # x_scaled: 0 ~ 1\n",
    "        x_scaled = x_scaled * (2**self.noise_model.num_bits) # x_scaled: 0 ~ noise model's max GL.\n",
    "        \n",
    "        kwargs['clean'] = x_scaled\n",
    "        with torch.no_grad(): \n",
    "            n = self.noise_model.sample(kwargs) # noisy image\n",
    "\n",
    "        n_scaled = n / (2**self.noise_model.num_bits) # n_scaled: 0 ~ 1\n",
    "        n_scaled = torch.clip(n_scaled, 0., 1.)\n",
    "        y = self.denoiser(n_scaled)\n",
    "        y = y * (2**self.num_bits) # y: 0 ~ denoiser's max GL.\n",
    "        return y\n",
    "    \n",
    "    def denoise(self, x, kwargs=None):\n",
    "        # x: noisy image\n",
    "        if kwargs is None or 'num_bits' not in kwargs: num_bits = self.num_bits\n",
    "        else: num_bits = kwargs['num_bits']\n",
    "\n",
    "        x_scaled = x / (2**num_bits) # x_scaled: 0 ~ 1\n",
    "        y =  self.denoiser(x_scaled) \n",
    "        y = torch.clip(y, 0., 1.)\n",
    "        y *= (2**num_bits) # x_scaled: 0 ~ denoiser's max GL.\n",
    "        return y\n",
    "    \n",
    "    def sample(self, x, kwargs=None):\n",
    "        # x: clean image\n",
    "        if kwargs is None or 'num_bits' not in kwargs: num_bits = self.num_bits\n",
    "        else: num_bits = kwargs['num_bits']\n",
    "\n",
    "        x_scaled = x / (2**num_bits) # x_scaled: 0 ~ 1\n",
    "        x_scaled = x_scaled * (2**self.noise_model.num_bits) # x_scaled: 0 ~ noise model's max GL.\n",
    "\n",
    "        kwargs = dict()\n",
    "        kwargs['clean'] = x_scaled\n",
    "        n = self.noise_model.sample(kwargs) # n: 0 ~ noise model's max GL.\n",
    "        \n",
    "        n_scaled = n / (2**self.noise_model.num_bits) # n_scaled: 0 ~ 1\n",
    "        n_scaled = n_scaled * (2**num_bits) # n_scaled: 0 ~ denoiser's max GL.\n",
    "        return n_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "n2m",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
