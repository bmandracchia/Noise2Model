# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/09_utils.ipynb.

# %% auto 0
__all__ = ['attributesFromDict', 'batch_PSNR', 'compute_index', 'compute_one_hot', 'StandardNormal', 'sum_except_batch']

# %% ../nbs/09_utils.ipynb 3
from fastai.vision.all import torch, nn
import numpy as np
from torchmetrics.functional.image import structural_similarity_index_measure as structural_similarity
from torchmetrics.functional.image import peak_signal_noise_ratio
import torch 
import torch.nn.functional as F

# %% ../nbs/09_utils.ipynb 4
def attributesFromDict(d):
    self = d.pop('self')
    for n, v in d.items():
        setattr(self, n, v)

# %% ../nbs/09_utils.ipynb 5
def batch_PSNR(img, imclean, data_range):
    Img = img.data.cpu().numpy().astype(np.float32)
    Iclean = imclean.data.cpu().numpy().astype(np.float32)
    PSNR = 0
    for i in range(Img.shape[0]):
        PSNR += peak_signal_noise_ratio(Iclean[i,:,:,:], Img[i,:,:,:], data_range=data_range)
    return (PSNR/Img.shape[0])

# %% ../nbs/09_utils.ipynb 7
class compute_index():
    def __init__(self, codes, device='cpu') -> None:
        attributesFromDict(locals( ))
      
    def _compute_index(self, b, **kwargs):
        idx = torch.zeros([b], device=self.device, dtype=torch.float32)
        for key, value in self.codes.items():
            idx = idx * len(value)
            for i, v in enumerate(value):
                idx += torch.where(kwargs[key] == v, i, 0.0)

        return idx
    
    def __call__(self, b, **kwargs):
        return self._compute_index(b, **kwargs)

# %% ../nbs/09_utils.ipynb 10
class compute_one_hot():
    def __init__(self, codes, device='cpu') -> None:
        attributesFromDict(locals( ))
      
    def _compute_one_hot(self, b, **kwargs):
        embedding = torch.tensor([])
        for key, value in self.codes.items():
            idx = torch.zeros([b], device=self.device, dtype=torch.float32)
            for i, v in enumerate(value):
                idx += torch.where(kwargs[key] == v, i, 0.0)
            idx_one_hot = F.one_hot(idx.to(torch.int64), num_classes=value.shape[0]).to(torch.float32)
            print(key, ': ', idx_one_hot)
            embedding = torch.cat((embedding, idx_one_hot), dim=1)

        return embedding
    
    def __call__(self, b, **kwargs):
        return self._compute_one_hot(b, **kwargs)

# %% ../nbs/09_utils.ipynb 13
import math
import torch
from torch import nn


# %% ../nbs/09_utils.ipynb 14
class StandardNormal(nn.Module):
    """A multivariate Normal with zero mean and unit covariance."""

    def __init__(self):
        super(StandardNormal, self).__init__()
        self.register_buffer('buffer', torch.zeros(1))

    def log_prob(self, x):
        # https://www.statlect.com/fundamentals-of-statistics/normal-distribution-maximum-likelihood
        log_base =  - 0.5 * math.log(2 * math.pi)
        log_inner = - 0.5 * x**2
        return sum_except_batch(log_base+log_inner)

    def sample(self, shape):
        return torch.randn(*shape, device=self.buffer.device, dtype=self.buffer.dtype)

def sum_except_batch(x, num_dims=1):
    '''
    Sums all dimensions except the first.
    Args:
        x: Tensor, shape (batch_size, ...)
        num_dims: int, number of batch dims (default=1)
    Returns:
        x_sum: Tensor, shape (batch_size,)
    '''
    return x.reshape(*x.shape[:num_dims], -1).sum(-1)
