{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NoiseFlow\n",
    "\n",
    "> noiseflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp noiseflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "\n",
    "# from fastai.vision.all import *\n",
    "# from fastai.data.all import *\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "from IPython.display import clear_output, DisplayHandle\n",
    "def update_patch(self, obj):\n",
    "    clear_output(wait=True)\n",
    "    self.display(obj)\n",
    "DisplayHandle.update = update_patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastai.vision.all import nn, torch\n",
    "import numpy as np\n",
    "\n",
    "from Noise2Model.utils import attributesFromDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from model.flow_layers.conv2d1x1 import Conv2d1x1\n",
    "from model.flow_layers.affine_coupling import AffineCoupling, ShiftAndLogScale\n",
    "from model.flow_layers.signal_dependant import SignalDependant\n",
    "from model.flow_layers.gain import Gain\n",
    "from model.flow_layers.utils import SdnModelScale\n",
    "\n",
    "\n",
    "class NoiseFlow(nn.Module):\n",
    "\n",
    "    def __init__(self, x_shape, arch, flow_permutation, param_inits, lu_decomp):\n",
    "        super(NoiseFlow, self).__init__()\n",
    "        attributesFromDict(locals( ))\n",
    "        self.model = nn.ModuleList(self.noise_flow_arch(x_shape))\n",
    "\n",
    "    def noise_flow_arch(self, x_shape):\n",
    "        arch_lyrs = self.arch.split('|')  # e.g., unc|sdn|unc|gain|unc\n",
    "        bijectors = []\n",
    "        for i, lyr in enumerate(arch_lyrs):\n",
    "            # is_last_layer = False\n",
    "\n",
    "            if lyr == 'unc':\n",
    "                if self.flow_permutation == 0:\n",
    "                    pass\n",
    "                elif self.flow_permutation == 1:\n",
    "                    print('|-Conv2d1x1')\n",
    "                    bijectors.append(\n",
    "                        Conv2d1x1(\n",
    "                            num_channels=x_shape[0],\n",
    "                            LU_decomposed=self.decomp,\n",
    "                            name='Conv2d_1x1_{}'.format(i)\n",
    "                        )\n",
    "                    )\n",
    "                else:\n",
    "                    print('|-No permutation specified. Not using any.')\n",
    "                    \n",
    "                print('|-AffineCoupling')\n",
    "                bijectors.append(\n",
    "                    AffineCoupling(\n",
    "                        x_shape=x_shape,\n",
    "                        shift_and_log_scale=ShiftAndLogScale,\n",
    "                        name='unc_%d' % i\n",
    "                    )\n",
    "                )\n",
    "            # elif lyr == 'lt':\n",
    "            #     print('|-LinearTransfomation')\n",
    "            #     bijectors.append(\n",
    "            #         LinearTransformation(\n",
    "            #             name='lt_{}'.format(i),\n",
    "            #             device='cuda'\n",
    "            #         )\n",
    "            #     )\n",
    "            elif lyr == 'sdn':\n",
    "                print('|-SignalDependant')\n",
    "                bijectors.append(\n",
    "                    SignalDependant(\n",
    "                        name='sdn_%d' % i,\n",
    "                        scale=SdnModelScale,\n",
    "                        param_inits=self.param_inits\n",
    "                    )\n",
    "                )\n",
    "            elif lyr == 'gain':\n",
    "                print('|-Gain')\n",
    "                bijectors.append(\n",
    "                    Gain(name='gain_%d' % i)\n",
    "                )\n",
    "\n",
    "        return bijectors\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        z = x\n",
    "        objective = torch.zeros(x.shape[0], dtype=torch.float32, device=x.device)\n",
    "        for bijector in self.model:\n",
    "            z, log_abs_det_J_inv = bijector._forward_and_log_det_jacobian(z, **kwargs)\n",
    "            objective += log_abs_det_J_inv\n",
    "\n",
    "            if 'writer' in kwargs.keys():\n",
    "                kwargs['writer'].add_scalar('model/' + bijector.name, torch.mean(log_abs_det_J_inv), kwargs['step'])\n",
    "        return z, objective\n",
    "\n",
    "    def _loss(self, x, **kwargs):\n",
    "        z, objective = self.forward(x, **kwargs)\n",
    "        # base measure\n",
    "        logp, _ = self.prior(\"prior\", x)\n",
    "\n",
    "        log_z = logp(z)\n",
    "        objective += log_z\n",
    "\n",
    "        if 'writer' in kwargs.keys():\n",
    "            kwargs['writer'].add_scalar('model/log_z', torch.mean(log_z), kwargs['step'])\n",
    "            kwargs['writer'].add_scalar('model/z', torch.mean(z), kwargs['step'])\n",
    "        nobj = - objective\n",
    "        # std. dev. of z\n",
    "        # mu_z = torch.mean(x, dim=[1, 2, 3])\n",
    "        var_z = torch.var(x, dim=[1, 2, 3])\n",
    "        sd_z = torch.mean(torch.sqrt(var_z))\n",
    "\n",
    "        return nobj, sd_z\n",
    "\n",
    "    def loss(self, x, **kwargs):\n",
    "        \n",
    "        # if 'writer' in kwargs.keys():\n",
    "        #     batch_average = torch.mean(x, dim=0)\n",
    "        #     kwargs['writer'].add_histogram('real_noise', batch_average, kwargs['step'])\n",
    "        #     kwargs['writer'].add_scalar('real_noise_std', torch.std(batch_average), kwargs['step'])\n",
    "\n",
    "        nll, sd_z = self._loss(x=x, **kwargs)\n",
    "        nll_dim = torch.mean(nll) / np.prod(x.shape[1:])\n",
    "        # nll_dim = torch.mean(nll)      # The above line should be uncommented\n",
    "\n",
    "        return nll_dim, sd_z\n",
    "\n",
    "    def inverse(self, z, **kwargs):\n",
    "        x = z\n",
    "        for bijector in reversed(self.model):\n",
    "            x = bijector._inverse(x, **kwargs)\n",
    "        return x\n",
    "    \n",
    "    def sample(self, eps_std=None, **kwargs):\n",
    "        _, sample = self.prior(\"prior\", kwargs['clean'])\n",
    "        z = sample(eps_std)\n",
    "        x = self.inverse(z, **kwargs)\n",
    "        batch_average = torch.mean(x, dim=0)\n",
    "        if 'writer' in kwargs.keys():\n",
    "            kwargs['writer'].add_histogram('sample_noise', batch_average, kwargs['step'])\n",
    "            kwargs['writer'].add_scalar('sample_noise_std', torch.std(batch_average), kwargs['step'])\n",
    "\n",
    "        return x\n",
    "\n",
    "    def prior(self, name, x):\n",
    "        n_z = x.shape[1]\n",
    "        h = torch.zeros([x.shape[0]] +  [2 * n_z] + list(x.shape[2:4]), device=x.device)\n",
    "        pz = gaussian_diag(h[:, :n_z, :, :], h[:, n_z:, :, :])\n",
    "\n",
    "        def logp(z1):\n",
    "            objective = pz.logp(z1)\n",
    "            return objective\n",
    "\n",
    "        def sample(eps_std=None):\n",
    "            if eps_std is not None:\n",
    "                z = pz.sample2(pz.eps * torch.reshape(eps_std, [-1, 1, 1, 1]))\n",
    "            else:\n",
    "                z = pz.sample\n",
    "            return z\n",
    "\n",
    "        return logp, sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def gaussian_diag(mean, logsd):\n",
    "    class o(object):\n",
    "        pass\n",
    "\n",
    "    o.mean = mean\n",
    "    o.logsd = logsd\n",
    "    o.eps = torch.normal(torch.zeros(mean.shape, device=mean.device), torch.ones(mean.shape, device=mean.device))\n",
    "    o.sample = mean + torch.exp(logsd) * o.eps\n",
    "    o.sample2 = lambda eps: mean + torch.exp(logsd) * eps\n",
    "\n",
    "    o.logps = lambda x: -0.5 * (np.log(2 * np.pi) + 2. * o.logsd + (x - o.mean) ** 2 / torch.exp(2. * o.logsd))\n",
    "    o.logp = lambda x: torch.sum(o.logps(x), dim=[1, 2, 3])\n",
    "    o.get_eps = lambda x: (x - mean) / torch.exp(logsd)\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
