{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NoiseFlow\n",
    "\n",
    "> noiseflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp noiseflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from IPython.display import clear_output, DisplayHandle\n",
    "\n",
    "def update_patch(self, obj):\n",
    "    clear_output(wait=True)\n",
    "    self.display(obj)\n",
    "DisplayHandle.update = update_patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from fastai.vision.all import nn, torch, np\n",
    "from Noise2Model.utils import attributesFromDict\n",
    "from Noise2Model.models import DnCNN, UNet\n",
    "from Noise2Model.utils import gaussian_diag, batch_PSNR, weights_init_orthogonal #, weights_init_kaiming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# from Noise2Model.layers.conv2d1x1 import Conv2d1x1\n",
    "# from Noise2Model.layers.affine_coupling import AffineCoupling, ShiftAndLogScale\n",
    "# from Noise2Model.layers.signal_dependant import SignalDependant\n",
    "# from Noise2Model.layers.gain import Gain\n",
    "# from Noise2Model.layers.utils import SdnModelScale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise Flow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class NoiseFlow(nn.Module):\n",
    "\n",
    "    def __init__(self, x_shape, arch, flow_permutation, param_inits, lu_decomp):\n",
    "        super(NoiseFlow, self).__init__()\n",
    "        attributesFromDict(locals( ))\n",
    "        self.model = nn.ModuleList(self.noise_flow_arch(x_shape))\n",
    "\n",
    "    def noise_flow_arch(self, x_shape):\n",
    "        arch_lyrs = self.arch.split('|')  # e.g., unc|sdn|unc|gain|unc\n",
    "        bijectors = []\n",
    "        for i, lyr in enumerate(arch_lyrs):\n",
    "            # is_last_layer = False\n",
    "\n",
    "            if lyr == 'unc':\n",
    "                if self.flow_permutation == 0:\n",
    "                    pass\n",
    "                elif self.flow_permutation == 1:\n",
    "                    print('|-Conv2d1x1')\n",
    "                    bijectors.append(\n",
    "                        Conv2d1x1(\n",
    "                            num_channels=x_shape[0],\n",
    "                            LU_decomposed=self.decomp,\n",
    "                            name='Conv2d_1x1_{}'.format(i)\n",
    "                        )\n",
    "                    )\n",
    "                else:\n",
    "                    print('|-No permutation specified. Not using any.')\n",
    "                    \n",
    "                print('|-AffineCoupling')\n",
    "                bijectors.append(\n",
    "                    AffineCoupling(\n",
    "                        x_shape=x_shape,\n",
    "                        shift_and_log_scale=ShiftAndLogScale,\n",
    "                        name='unc_%d' % i\n",
    "                    )\n",
    "                )\n",
    "            # elif lyr == 'lt':\n",
    "            #     print('|-LinearTransfomation')\n",
    "            #     bijectors.append(\n",
    "            #         LinearTransformation(\n",
    "            #             name='lt_{}'.format(i),\n",
    "            #             device='cuda'\n",
    "            #         )\n",
    "            #     )\n",
    "            elif lyr == 'sdn':\n",
    "                print('|-SignalDependant')\n",
    "                bijectors.append(\n",
    "                    SignalDependant(\n",
    "                        name='sdn_%d' % i,\n",
    "                        scale=SdnModelScale,\n",
    "                        param_inits=self.param_inits\n",
    "                    )\n",
    "                )\n",
    "            elif lyr == 'gain':\n",
    "                print('|-Gain')\n",
    "                bijectors.append(\n",
    "                    Gain(name='gain_%d' % i)\n",
    "                )\n",
    "\n",
    "        return bijectors\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        z = x\n",
    "        objective = torch.zeros(x.shape[0], dtype=torch.float32, device=x.device)\n",
    "        for bijector in self.model:\n",
    "            z, log_abs_det_J_inv = bijector._forward_and_log_det_jacobian(z, **kwargs)\n",
    "            objective += log_abs_det_J_inv\n",
    "\n",
    "            if 'writer' in kwargs.keys():\n",
    "                kwargs['writer'].add_scalar('model/' + bijector.name, torch.mean(log_abs_det_J_inv), kwargs['step'])\n",
    "        return z, objective\n",
    "\n",
    "    def _loss(self, x, **kwargs):\n",
    "        z, objective = self.forward(x, **kwargs)\n",
    "        # base measure\n",
    "        logp, _ = self.prior(\"prior\", x)\n",
    "\n",
    "        log_z = logp(z)\n",
    "        objective += log_z\n",
    "\n",
    "        if 'writer' in kwargs.keys():\n",
    "            kwargs['writer'].add_scalar('model/log_z', torch.mean(log_z), kwargs['step'])\n",
    "            kwargs['writer'].add_scalar('model/z', torch.mean(z), kwargs['step'])\n",
    "        nobj = - objective\n",
    "        # std. dev. of z\n",
    "        # mu_z = torch.mean(x, dim=[1, 2, 3])\n",
    "        var_z = torch.var(x, dim=[1, 2, 3])\n",
    "        sd_z = torch.mean(torch.sqrt(var_z))\n",
    "\n",
    "        return nobj, sd_z\n",
    "\n",
    "    def loss(self, x, **kwargs):\n",
    "        \n",
    "        # if 'writer' in kwargs.keys():\n",
    "        #     batch_average = torch.mean(x, dim=0)\n",
    "        #     kwargs['writer'].add_histogram('real_noise', batch_average, kwargs['step'])\n",
    "        #     kwargs['writer'].add_scalar('real_noise_std', torch.std(batch_average), kwargs['step'])\n",
    "\n",
    "        nll, sd_z = self._loss(x=x, **kwargs)\n",
    "        nll_dim = torch.mean(nll) / np.prod(x.shape[1:])\n",
    "        # nll_dim = torch.mean(nll)      # The above line should be uncommented\n",
    "\n",
    "        return nll_dim, sd_z\n",
    "\n",
    "    def inverse(self, z, **kwargs):\n",
    "        x = z\n",
    "        for bijector in reversed(self.model):\n",
    "            x = bijector._inverse(x, **kwargs)\n",
    "        return x\n",
    "    \n",
    "    def sample(self, eps_std=None, **kwargs):\n",
    "        _, sample = self.prior(\"prior\", kwargs['clean'])\n",
    "        z = sample(eps_std)\n",
    "        x = self.inverse(z, **kwargs)\n",
    "        batch_average = torch.mean(x, dim=0)\n",
    "        if 'writer' in kwargs.keys():\n",
    "            kwargs['writer'].add_histogram('sample_noise', batch_average, kwargs['step'])\n",
    "            kwargs['writer'].add_scalar('sample_noise_std', torch.std(batch_average), kwargs['step'])\n",
    "\n",
    "        return x\n",
    "\n",
    "    def prior(self, name, x):\n",
    "        n_z = x.shape[1]\n",
    "        h = torch.zeros([x.shape[0]] +  [2 * n_z] + list(x.shape[2:4]), device=x.device)\n",
    "        pz = gaussian_diag(h[:, :n_z, :, :], h[:, n_z:, :, :])\n",
    "\n",
    "        def logp(z1):\n",
    "            objective = pz.logp(z1)\n",
    "            return objective\n",
    "\n",
    "        def sample(eps_std=None):\n",
    "            if eps_std is not None:\n",
    "                z = pz.sample2(pz.eps * torch.reshape(eps_std, [-1, 1, 1, 1]))\n",
    "            else:\n",
    "                z = pz.sample\n",
    "            return z\n",
    "\n",
    "        return logp, sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch import randn as torch_randn\n",
    "# from fastai.vision.all import test_eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|-Gain\n",
      "[ModuleList(\n",
      "  (0): Gain()\n",
      ")]\n",
      "torch.Size([16, 1, 64, 64])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "# def init_params():\n",
    "#     npcam = 3\n",
    "#     c_i = 1.0\n",
    "#     beta1_i = -5.0 / c_i\n",
    "#     beta2_i = 0.0\n",
    "#     gain_params_i = np.ndarray([5])\n",
    "#     gain_params_i[:] = -5.0 / c_i\n",
    "#     cam_params_i = np.ndarray([npcam, 5])\n",
    "#     cam_params_i[:, :] = 1.0\n",
    "#     return (c_i, beta1_i, beta2_i, gain_params_i, cam_params_i)\n",
    "\n",
    "# x = torch_randn(16,1,64,64)\n",
    "# xdim = len(x.shape)-2\n",
    "\n",
    "# tst = NoiseFlow(x.shape[1:], arch='gain', flow_permutation=0, param_inits=init_params(), lu_decomp=0)\n",
    "# mods = list(tst.children())\n",
    "# print(mods)\n",
    "# # test_eq(tst(x.cuda()).shape, [16, 1, 32, 64, 64])\n",
    "# logp, sample = tst(x.cuda())\n",
    "# print(logp.shape)\n",
    "# print(sample.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "n2m",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
